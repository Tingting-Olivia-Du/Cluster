#!/usr/bin/env python3
"""
AIMEæ•°æ®é›†å‡†ç¡®æ€§åˆ†æè„šæœ¬
æ¯”è¾ƒæ¯ä¸ªé—®é¢˜çš„true_final_resultå’Œæ¯ä¸ªsampleçš„ç­”æ¡ˆ
"""

import json
import os
from pathlib import Path
from typing import Dict, List, Tuple

def is_correct_answer(final_result, true_final_result):
    """
    åˆ¤æ–­final_resultæ˜¯å¦ä¸true_final_resultåŒ¹é…
    """
    return str(final_result).strip() == str(true_final_result).strip()

def analyze_aime_accuracy(input_json_path: str, output_path: str = None):
    """
    åˆ†æAIMEæ•°æ®é›†çš„å‡†ç¡®æ€§
    
    Args:
        input_json_path: è¾“å…¥çš„JSONæ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    """
    
    print("ğŸ” Loading AIME dataset...")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(input_json_path):
        print(f"âŒ Error: File {input_json_path} not found!")
        return
    
    # åŠ è½½æ•°æ®
    with open(input_json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"ğŸ“Š Loaded {len(data)} problems")
    
    # ç»Ÿè®¡å˜é‡
    total_problems = len(data)
    correct_samples = {"sampling0": 0, "sampling1": 0, "sampling2": 0}
    total_samples = {"sampling0": 0, "sampling1": 0, "sampling2": 0}
    
    # è¯¦ç»†ç»“æœ
    detailed_results = []
    
    print("\n" + "="*80)
    print("AIME ACCURACY ANALYSIS")
    print("="*80)
    
    for qid, sample in data.items():
        print(f"\nğŸ” Analyzing {qid}...")
        
        # è·å–åŸºæœ¬ä¿¡æ¯
        original_data = sample.get("original_data", {})
        question_id = original_data.get("ID", "Unknown")
        year = original_data.get("Year", "Unknown")
        problem_num = original_data.get("Problem Number", "Unknown")
        question_text = original_data.get("Question", "")[:100] + "..." if len(original_data.get("Question", "")) > 100 else original_data.get("Question", "")
        
        true_final_result = sample.get("true_final_result", "")
        
        print(f"  ğŸ“‹ Question ID: {question_id}")
        print(f"  ğŸ“… Year: {year}, Problem: {problem_num}")
        print(f"  âœ… True Answer: {true_final_result}")
        print(f"  ğŸ“ Question: {question_text}")
        
        problem_result = {
            "qid": qid,
            "question_id": question_id,
            "year": year,
            "problem_num": problem_num,
            "true_answer": true_final_result,
            "question_text": original_data.get("Question", ""),
            "samples": {}
        }
        
        # åˆ†ææ¯ä¸ªsample
        for sampling_id in ["sampling0", "sampling1", "sampling2"]:
            if sampling_id not in sample:
                print(f"    âš ï¸ {sampling_id}: Not found")
                continue
            
            sampling_data = sample[sampling_id]
            final_result = sampling_data.get("final_result", "")
            is_correct = is_correct_answer(final_result, true_final_result)
            
            # æ›´æ–°ç»Ÿè®¡
            total_samples[sampling_id] += 1
            if is_correct:
                correct_samples[sampling_id] += 1
            
            # è®°å½•ç»“æœ
            problem_result["samples"][sampling_id] = {
                "final_result": final_result,
                "is_correct": is_correct,
                "whole_answer": sampling_data.get("whole_answer", "")[:200] + "..." if len(sampling_data.get("whole_answer", "")) > 200 else sampling_data.get("whole_answer", "")
            }
            
            status = "âœ…" if is_correct else "âŒ"
            print(f"    {status} {sampling_id}: {final_result} (Expected: {true_final_result})")
        
        detailed_results.append(problem_result)
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    print("\n" + "="*80)
    print("OVERALL STATISTICS")
    print("="*80)
    
    overall_stats = {}
    for sampling_id in ["sampling0", "sampling1", "sampling2"]:
        if total_samples[sampling_id] > 0:
            accuracy = (correct_samples[sampling_id] / total_samples[sampling_id]) * 100
            overall_stats[sampling_id] = {
                "total_samples": total_samples[sampling_id],
                "correct_samples": correct_samples[sampling_id],
                "accuracy": accuracy
            }
            print(f"ğŸ“Š {sampling_id}:")
            print(f"   Total: {total_samples[sampling_id]}")
            print(f"   Correct: {correct_samples[sampling_id]}")
            print(f"   Accuracy: {accuracy:.2f}%")
    
    # è®¡ç®—å¹³å‡å‡†ç¡®ç‡
    if overall_stats:
        avg_accuracy = sum(stats["accuracy"] for stats in overall_stats.values()) / len(overall_stats)
        print(f"\nğŸ“ˆ Average Accuracy: {avg_accuracy:.2f}%")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    if output_path:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        result_data = {
            "summary": {
                "total_problems": total_problems,
                "overall_stats": overall_stats,
                "average_accuracy": avg_accuracy if overall_stats else 0
            },
            "detailed_results": detailed_results
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ Detailed results saved to: {output_path}")
    
    # ç”Ÿæˆé”™è¯¯åˆ†ææŠ¥å‘Š
    print("\n" + "="*80)
    print("ERROR ANALYSIS")
    print("="*80)
    
    error_analysis = {}
    for sampling_id in ["sampling0", "sampling1", "sampling2"]:
        error_analysis[sampling_id] = []
        
        for problem in detailed_results:
            if sampling_id in problem["samples"]:
                sample = problem["samples"][sampling_id]
                if not sample["is_correct"]:
                    error_analysis[sampling_id].append({
                        "qid": problem["qid"],
                        "question_id": problem["question_id"],
                        "true_answer": problem["true_answer"],
                        "wrong_answer": sample["final_result"],
                        "question_text": problem["question_text"][:100] + "..." if len(problem["question_text"]) > 100 else problem["question_text"]
                    })
    
    for sampling_id, errors in error_analysis.items():
        print(f"\nâŒ {sampling_id} Errors ({len(errors)}):")
        for error in errors[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
            print(f"   {error['question_id']}: Expected {error['true_answer']}, Got {error['wrong_answer']}")
        if len(errors) > 5:
            print(f"   ... and {len(errors) - 5} more errors")
    
    return result_data

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ AIME Dataset Accuracy Analyzer")
    print("="*50)
    
    # é…ç½®å‚æ•°
    START_YEAR = 2022
    END_YEAR = 2023
    MIN_PROBLEM_NUM = 1
    MAX_PROBLEM_NUM = 5
    SAMPLE_SIZE = 10
    
    input_json = f"./logits/aime_experiment_{MIN_PROBLEM_NUM}-{MAX_PROBLEM_NUM}_{START_YEAR}-{END_YEAR}_sample{SAMPLE_SIZE}.json"
    output_results = f"./output/aime_accuracy_analysis_{MIN_PROBLEM_NUM}-{MAX_PROBLEM_NUM}_{START_YEAR}-{END_YEAR}_sample{SAMPLE_SIZE}.json"
    
    # è¿è¡Œåˆ†æ
    results = analyze_aime_accuracy(input_json, output_results)
    
    print(f"\nâœ… Analysis complete!")
    print(f"ğŸ“Š Results saved to: {output_results}")

if __name__ == "__main__":
    main() 
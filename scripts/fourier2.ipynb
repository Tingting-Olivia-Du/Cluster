{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMJKVjyXzeIp1vuuOLjyFla"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"XFafq1fspDLm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752394397110,"user_tz":-480,"elapsed":2837,"user":{"displayName":"Tingting Du","userId":"01262363838823204487"}},"outputId":"b1cebd34-79fd-4560-e9a7-2fdacadc55e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Fourier analysis comparing positive/negative samples with error and fix positions\n","\"\"\"\n","\n","import os\n","import json\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.fft import fft, fftfreq, ifft\n","from google.colab import drive\n","\n","# ✅ 挂载 Google Drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Memory-efficient Fourier analysis using streaming processing\n","\"\"\"\n","\n","import os\n","import json\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.fft import fft, fftfreq, ifft\n","import gc\n","from google.colab import drive\n","\n","# ✅ 挂载 Google Drive\n","# drive.mount('/content/drive')\n","\n","# ✅ 配置路径参数\n","start_index = 700\n","end_index = 731\n","range_tag = f\"{start_index}-{end_index}\"\n","BASE_PATH = \"/content/drive/MyDrive/Cluster-proj\"\n","\n","# 输入文件路径\n","LOGITS_JSONL_PATH = f\"{BASE_PATH}/output/llm_steps/whole_logits/deepseek7b-gsm-{range_tag}.jsonl\"\n","ERROR_FIX_INDEX_PATH = f\"{BASE_PATH}/output/error_fix_index/deepseek-7b-{range_tag}_error_fix_index.json\"\n","\n","# ✅ 检查JSONL文件是否存在\n","if not os.path.exists(LOGITS_JSONL_PATH):\n","    print(f\"⚠️ JSONL file not found: {LOGITS_JSONL_PATH}\")\n","    print(\"Converting JSON to JSONL first...\")\n","\n","    # 快速转换JSON到JSONL\n","    json_path = f\"{BASE_PATH}/output/llm_steps/whole_logits/deepseek-math-7b-gsm-{range_tag}.json\"\n","\n","    def convert_json_to_jsonl_chunked(json_path, jsonl_path, chunk_size=10):\n","        \"\"\"分块转换JSON到JSONL以节省内存\"\"\"\n","        print(f\"🔄 Converting {json_path} to JSONL...\")\n","\n","        with open(json_path, 'r', encoding='utf-8') as f:\n","            data = json.load(f)\n","\n","        total_items = len(data)\n","        print(f\"📊 Total items: {total_items}\")\n","\n","        with open(jsonl_path, 'w', encoding='utf-8') as f:\n","            for i, (qid, sample) in enumerate(data.items()):\n","                line_data = {\"qid\": qid, \"data\": sample}\n","                f.write(json.dumps(line_data, ensure_ascii=False) + '\\n')\n","\n","                if (i + 1) % chunk_size == 0:\n","                    print(f\"📈 Converted {i + 1}/{total_items}\")\n","                    f.flush()  # 强制写入磁盘\n","\n","        # 清理内存\n","        del data\n","        gc.collect()\n","        print(f\"✅ Conversion complete: {jsonl_path}\")\n","\n","    convert_json_to_jsonl_chunked(json_path, LOGITS_JSONL_PATH)\n","\n","# ✅ 加载错误修复索引数据\n","print(\"📂 Loading error-fix index data...\")\n","if os.path.exists(ERROR_FIX_INDEX_PATH):\n","    with open(ERROR_FIX_INDEX_PATH, \"r\") as f:\n","        error_fix_data = json.load(f)\n","    print(f\"✅ Loaded error-fix data: {len(error_fix_data)} questions\")\n","else:\n","    print(f\"⚠️ Error-fix index file not found: {ERROR_FIX_INDEX_PATH}\")\n","    print(\"Please run the error analysis script first!\")\n","    exit()\n","\n","# ✅ 输出路径\n","OUTPUT_DIR = f\"{BASE_PATH}/output/fourier_analysis_error_fix\"\n","DETAIL_DIR = f\"{BASE_PATH}/output/fourier_analysis_detail\"\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","os.makedirs(DETAIL_DIR, exist_ok=True)\n","\n","# ✅ 傅里叶平滑函数\n","def fourier_smooth(y, keep_ratio=0.1):\n","    \"\"\"傅里叶变换平滑函数\"\"\"\n","    y = np.asarray(y)\n","    N = len(y)\n","    if N < 4:\n","        return y\n","    Y = fft(y)\n","    Y[int(N * keep_ratio):-int(N * keep_ratio)] = 0\n","    y_smooth = np.real(ifft(Y))\n","    return y_smooth\n","\n","# ✅ 流式加载单个样本的logits数据\n","def load_sample_logits(qid, sampling_ids):\n","    \"\"\"\n","    从JSONL文件中流式加载指定样本的logits数据\n","    \"\"\"\n","    sample_data = {}\n","\n","    with open(LOGITS_JSONL_PATH, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            line_data = json.loads(line.strip())\n","            if line_data[\"qid\"] == qid:\n","                data = line_data[\"data\"]\n","                # 只提取需要的sampling数据\n","                for sid in sampling_ids:\n","                    if sid in data:\n","                        sample_data[sid] = data[sid]\n","                break\n","\n","    return sample_data\n","\n","# ✅ 绘制熵曲线对比（内存优化版本）\n","def plot_entropy_with_error_fix_positions_efficient(qid, neg_sid, pos_sid, analysis_data, save_dir):\n","    \"\"\"\n","    内存效率优化的熵曲线绘制函数\n","    \"\"\"\n","    try:\n","        # 流式加载所需的样本数据\n","        sample_logits = load_sample_logits(qid, [neg_sid, pos_sid])\n","\n","        if neg_sid not in sample_logits or pos_sid not in sample_logits:\n","            print(f\"⚠️ 缺失 logits：{qid} | {neg_sid} / {pos_sid}\")\n","            return\n","\n","        neg_probs = sample_logits[neg_sid][\"token_probs\"]\n","        pos_probs = sample_logits[pos_sid][\"token_probs\"]\n","\n","        # 提取熵值\n","        entropy_neg = np.array([tok[\"topk_info\"][\"entropy\"] for tok in neg_probs])\n","        entropy_pos = np.array([tok[\"topk_info\"][\"entropy\"] for tok in pos_probs])\n","\n","        if len(entropy_neg) < 4 or len(entropy_pos) < 4:\n","            print(f\"⚠️ 序列太短，跳过 {qid}\")\n","            return\n","\n","        idx_neg = np.arange(len(entropy_neg))\n","        idx_pos = np.arange(len(entropy_pos))\n","\n","        # 傅里叶平滑\n","        smooth_neg = fourier_smooth(entropy_neg, keep_ratio=0.1)\n","        smooth_pos = fourier_smooth(entropy_pos, keep_ratio=0.1)\n","\n","        # 错误和修复位置\n","        error_begin = analysis_data.get(\"error_token_begin_index\", -1)\n","        error_end = analysis_data.get(\"error_token_end_index\", -1)\n","        fix_begin = analysis_data.get(\"fix_token_begin_index\", -1)\n","        fix_end = analysis_data.get(\"fix_token_end_index\", -1)\n","\n","        # ✅ 创建图形\n","        fig, axs = plt.subplots(2, 2, figsize=(16, 10))\n","\n","        # 子图1: 负样本（错误）的熵曲线\n","        axs[0, 0].plot(idx_neg, entropy_neg, 'o-', alpha=0.3, label=\"Original Entropy\", color='red')\n","        axs[0, 0].plot(idx_neg, smooth_neg, '-', label=\"Smoothed Entropy\", color='red', linewidth=2)\n","\n","        # 标注错误位置\n","        if error_begin >= 0 and error_end >= 0:\n","            axs[0, 0].axvspan(error_begin, error_end, alpha=0.2, color='red', label=\"Error Region\")\n","            axs[0, 0].axvline(error_begin, color='red', linestyle='--', alpha=0.7)\n","            axs[0, 0].axvline(error_end, color='red', linestyle='--', alpha=0.7)\n","\n","        axs[0, 0].set_title(f\"Negative Sample ({neg_sid}) - Error Position\")\n","        axs[0, 0].set_xlabel(\"Token Index\")\n","        axs[0, 0].set_ylabel(\"Entropy\")\n","        axs[0, 0].legend()\n","        axs[0, 0].grid(True)\n","\n","        # 子图2: 正样本（正确）的熵曲线\n","        axs[0, 1].plot(idx_pos, entropy_pos, 'o-', alpha=0.3, label=\"Original Entropy\", color='green')\n","        axs[0, 1].plot(idx_pos, smooth_pos, '-', label=\"Smoothed Entropy\", color='green', linewidth=2)\n","\n","        # 标注修复位置\n","        if fix_begin >= 0 and fix_end >= 0:\n","            axs[0, 1].axvspan(fix_begin, fix_end, alpha=0.2, color='blue', label=\"Fix Region\")\n","            axs[0, 1].axvline(fix_begin, color='blue', linestyle='--', alpha=0.7)\n","            axs[0, 1].axvline(fix_end, color='blue', linestyle='--', alpha=0.7)\n","\n","        axs[0, 1].set_title(f\"Positive Sample ({pos_sid}) - Fix Position\")\n","        axs[0, 1].set_xlabel(\"Token Index\")\n","        axs[0, 1].set_ylabel(\"Entropy\")\n","        axs[0, 1].legend()\n","        axs[0, 1].grid(True)\n","\n","        # 子图3: 对比图（重叠显示）\n","        axs[1, 0].plot(idx_neg, smooth_neg, '-', label=f\"{neg_sid} (Error)\", color='red', linewidth=2)\n","        axs[1, 0].plot(idx_pos, smooth_pos, '-', label=f\"{pos_sid} (Correct)\", color='green', linewidth=2)\n","\n","        # 同时标注错误和修复位置\n","        if error_begin >= 0 and error_end >= 0:\n","            axs[1, 0].axvspan(error_begin, error_end, alpha=0.15, color='red', label=\"Error Region\")\n","        if fix_begin >= 0 and fix_end >= 0:\n","            axs[1, 0].axvspan(fix_begin, fix_end, alpha=0.15, color='blue', label=\"Fix Region\")\n","\n","        axs[1, 0].set_title(\"Positive vs Negative Sample Comparison\")\n","        axs[1, 0].set_xlabel(\"Token Index\")\n","        axs[1, 0].set_ylabel(\"Entropy\")\n","        axs[1, 0].legend()\n","        axs[1, 0].grid(True)\n","\n","        # 子图4: 频谱分析\n","        N_neg = len(entropy_neg)\n","        N_pos = len(entropy_pos)\n","\n","        freqs_neg = fftfreq(N_neg, d=1)[:N_neg // 2]\n","        amp_neg = np.abs(fft(entropy_neg))[:N_neg // 2]\n","\n","        freqs_pos = fftfreq(N_pos, d=1)[:N_pos // 2]\n","        amp_pos = np.abs(fft(entropy_pos))[:N_pos // 2]\n","\n","        axs[1, 1].plot(freqs_neg, amp_neg, label=f\"{neg_sid} Spectrum\", color='red')\n","        axs[1, 1].plot(freqs_pos, amp_pos, label=f\"{pos_sid} Spectrum\", color='green')\n","        axs[1, 1].set_title(\"Fourier Amplitude Spectrum Comparison\")\n","        axs[1, 1].set_xlabel(\"Frequency\")\n","        axs[1, 1].set_ylabel(\"Amplitude\")\n","        axs[1, 1].legend()\n","        axs[1, 1].grid(True)\n","\n","        # 设置整体标题\n","        plt.suptitle(f\"Entropy Analysis: {qid} | {neg_sid} vs {pos_sid}\", fontsize=14)\n","        plt.tight_layout()\n","\n","        # 保存图片\n","        fname = f\"{qid}_{neg_sid}_vs_{pos_sid}_error_fix_analysis.png\"\n","        plt.savefig(os.path.join(save_dir, fname), dpi=150, bbox_inches='tight')\n","        plt.close()\n","\n","        # 清理内存\n","        del sample_logits, neg_probs, pos_probs, entropy_neg, entropy_pos\n","        gc.collect()\n","\n","        print(f\"✅ Saved: {fname}\")\n","\n","    except Exception as e:\n","        print(f\"❌ Processing failed {qid} | {neg_sid} vs {pos_sid}: {e}\")\n","\n","# ✅ 生成详细的分析报告图（内存优化版本）\n","def plot_detailed_position_analysis_efficient(qid, neg_sid, pos_sid, analysis_data, save_dir):\n","    \"\"\"\n","    内存效率优化的详细位置分析图\n","    \"\"\"\n","    try:\n","        # 流式加载所需的样本数据\n","        sample_logits = load_sample_logits(qid, [neg_sid, pos_sid])\n","\n","        if neg_sid not in sample_logits or pos_sid not in sample_logits:\n","            return\n","\n","        neg_probs = sample_logits[neg_sid][\"token_probs\"]\n","        pos_probs = sample_logits[pos_sid][\"token_probs\"]\n","\n","        entropy_neg = np.array([tok[\"topk_info\"][\"entropy\"] for tok in neg_probs])\n","        entropy_pos = np.array([tok[\"topk_info\"][\"entropy\"] for tok in pos_probs])\n","\n","        # 提取错误和修复区间的熵值\n","        error_begin = analysis_data.get(\"error_token_begin_index\", -1)\n","        error_end = analysis_data.get(\"error_token_end_index\", -1)\n","        fix_begin = analysis_data.get(\"fix_token_begin_index\", -1)\n","        fix_end = analysis_data.get(\"fix_token_end_index\", -1)\n","\n","        fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n","\n","        # 子图1: 错误区间详细分析\n","        if error_begin >= 0 and error_end >= 0 and error_end < len(entropy_neg):\n","            error_entropy = entropy_neg[error_begin:error_end+1]\n","            error_indices = np.arange(error_begin, error_end+1)\n","\n","            axs[0].plot(error_indices, error_entropy, 'ro-', linewidth=2, markersize=6)\n","            axs[0].set_title(\"Error Region Entropy Details\")\n","            axs[0].set_xlabel(\"Token Index\")\n","            axs[0].set_ylabel(\"Entropy\")\n","            axs[0].grid(True)\n","\n","            # 添加token信息（限制数量以避免拥挤）\n","            max_annotations = min(10, len(error_indices))\n","            step = max(1, len(error_indices) // max_annotations)\n","            for i in range(0, len(error_indices), step):\n","                idx = error_indices[i]\n","                if idx < len(neg_probs):\n","                    token_text = neg_probs[idx][\"token\"][:10]  # 限制长度\n","                    axs[0].annotate(f\"{token_text}\", (idx, error_entropy[i]),\n","                                  textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=8)\n","\n","        # 子图2: 修复区间详细分析\n","        if fix_begin >= 0 and fix_end >= 0 and fix_end < len(entropy_pos):\n","            fix_entropy = entropy_pos[fix_begin:fix_end+1]\n","            fix_indices = np.arange(fix_begin, fix_end+1)\n","\n","            axs[1].plot(fix_indices, fix_entropy, 'go-', linewidth=2, markersize=6)\n","            axs[1].set_title(\"Fix Region Entropy Details\")\n","            axs[1].set_xlabel(\"Token Index\")\n","            axs[1].set_ylabel(\"Entropy\")\n","            axs[1].grid(True)\n","\n","            # 添加token信息\n","            max_annotations = min(10, len(fix_indices))\n","            step = max(1, len(fix_indices) // max_annotations)\n","            for i in range(0, len(fix_indices), step):\n","                idx = fix_indices[i]\n","                if idx < len(pos_probs):\n","                    token_text = pos_probs[idx][\"token\"][:10]  # 限制长度\n","                    axs[1].annotate(f\"{token_text}\", (idx, fix_entropy[i]),\n","                                  textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=8)\n","\n","        # 子图3: 错误vs修复区间对比\n","        if (error_begin >= 0 and error_end >= 0 and error_end < len(entropy_neg) and\n","            fix_begin >= 0 and fix_end >= 0 and fix_end < len(entropy_pos)):\n","\n","            error_entropy = entropy_neg[error_begin:error_end+1]\n","            fix_entropy = entropy_pos[fix_begin:fix_end+1]\n","\n","            # 对齐长度进行对比\n","            min_len = min(len(error_entropy), len(fix_entropy))\n","            if min_len > 0:\n","                x_indices = np.arange(min_len)\n","                axs[2].plot(x_indices, error_entropy[:min_len], 'ro-', label=\"Error Region\", linewidth=2)\n","                axs[2].plot(x_indices, fix_entropy[:min_len], 'go-', label=\"Fix Region\", linewidth=2)\n","                axs[2].set_title(\"Error vs Fix Region Comparison\")\n","                axs[2].set_xlabel(\"Relative Position\")\n","                axs[2].set_ylabel(\"Entropy\")\n","                axs[2].legend()\n","                axs[2].grid(True)\n","\n","        plt.suptitle(f\"Position Analysis: {qid} | Error vs Fix\", fontsize=14)\n","        plt.tight_layout()\n","\n","        fname = f\"{qid}_{neg_sid}_vs_{pos_sid}_position_detail.png\"\n","        plt.savefig(os.path.join(save_dir, fname), dpi=150, bbox_inches='tight')\n","        plt.close()\n","\n","        # 清理内存\n","        del sample_logits, neg_probs, pos_probs, entropy_neg, entropy_pos\n","        gc.collect()\n","\n","    except Exception as e:\n","        print(f\"❌ Detailed analysis failed {qid}: {e}\")\n","\n","# ✅ 主处理循环（内存优化）\n","print(\"\\n🚀 Starting analysis generation...\")\n","\n","# 统计信息\n","total_samples = 0\n","processed_count = 0\n","error_count = 0\n","\n","# 逐个处理样本，避免内存累积\n","for qid, sample_data in error_fix_data.items():\n","    for neg_sid, analysis in sample_data.items():\n","        pos_sid = analysis.get(\"correct_sampling_id\")\n","        if pos_sid:\n","            total_samples += 1\n","\n","            try:\n","                print(f\"🔍 Processing {total_samples}: {qid} | {neg_sid} vs {pos_sid}\")\n","\n","                # 生成主要分析图\n","                plot_entropy_with_error_fix_positions_efficient(\n","                    qid, neg_sid, pos_sid, analysis, OUTPUT_DIR\n","                )\n","\n","                # 生成详细位置分析图\n","                plot_detailed_position_analysis_efficient(\n","                    qid, neg_sid, pos_sid, analysis, DETAIL_DIR\n","                )\n","\n","                processed_count += 1\n","\n","                # 定期清理内存\n","                if total_samples % 5 == 0:\n","                    gc.collect()\n","                    print(f\"📊 Processed {processed_count}/{total_samples} samples\")\n","\n","            except Exception as e:\n","                print(f\"❌ Processing failed {qid} | {neg_sid} vs {pos_sid}: {e}\")\n","                error_count += 1\n","                continue\n","\n","# ✅ 生成最终统计报告\n","print(f\"\\n🎉 Analysis completed!\")\n","print(f\"📊 Total samples: {total_samples}\")\n","print(f\"✅ Successfully processed: {processed_count}\")\n","print(f\"❌ Failed to process: {error_count}\")\n","print(f\"📁 Main analysis plots saved to: {OUTPUT_DIR}\")\n","print(f\"📁 Detailed analysis plots saved to: {DETAIL_DIR}\")\n","\n","# ✅ 生成处理摘要\n","summary_data = {\n","    \"processing_summary\": {\n","        \"total_samples\": total_samples,\n","        \"successful_processed\": processed_count,\n","        \"failed_processed\": error_count,\n","        \"success_rate\": f\"{processed_count/total_samples*100:.1f}%\" if total_samples > 0 else \"0%\"\n","    },\n","    \"output_directories\": {\n","        \"main_analysis\": OUTPUT_DIR,\n","        \"detailed_analysis\": DETAIL_DIR\n","    }\n","}\n","\n","summary_file = os.path.join(OUTPUT_DIR, \"processing_summary.json\")\n","with open(summary_file, 'w', encoding='utf-8') as f:\n","    json.dump(summary_data, f, ensure_ascii=False, indent=2)\n","\n","print(f\"📄 Processing summary saved to: {summary_file}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Hf8U7qOpVym","executionInfo":{"status":"ok","timestamp":1752395295646,"user_tz":-480,"elapsed":797098,"user":{"displayName":"Tingting Du","userId":"01262363838823204487"}},"outputId":"af6f3f68-88cf-41c9-80d9-a9ef0baa6bc9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["📂 Loading error-fix index data...\n","✅ Loaded error-fix data: 19 questions\n","\n","🚀 Starting analysis generation...\n","🔍 Processing 1: q_700 | sampling0 vs sampling1\n","✅ Saved: q_700_sampling0_vs_sampling1_error_fix_analysis.png\n","🔍 Processing 2: q_700 | sampling2 vs sampling1\n","✅ Saved: q_700_sampling2_vs_sampling1_error_fix_analysis.png\n","🔍 Processing 3: q_701 | sampling1 vs sampling0\n","✅ Saved: q_701_sampling1_vs_sampling0_error_fix_analysis.png\n","🔍 Processing 4: q_703 | sampling1 vs sampling0\n","✅ Saved: q_703_sampling1_vs_sampling0_error_fix_analysis.png\n","🔍 Processing 5: q_705 | sampling1 vs sampling0\n","✅ Saved: q_705_sampling1_vs_sampling0_error_fix_analysis.png\n","📊 Processed 5/5 samples\n","🔍 Processing 6: q_705 | sampling2 vs sampling0\n","✅ Saved: q_705_sampling2_vs_sampling0_error_fix_analysis.png\n","🔍 Processing 7: q_707 | sampling0 vs sampling1\n","✅ Saved: q_707_sampling0_vs_sampling1_error_fix_analysis.png\n","🔍 Processing 8: q_707 | sampling2 vs sampling1\n","✅ Saved: q_707_sampling2_vs_sampling1_error_fix_analysis.png\n","🔍 Processing 9: q_709 | sampling0 vs sampling1\n","✅ Saved: q_709_sampling0_vs_sampling1_error_fix_analysis.png\n","🔍 Processing 10: q_709 | sampling2 vs sampling1\n","✅ Saved: q_709_sampling2_vs_sampling1_error_fix_analysis.png\n","📊 Processed 10/10 samples\n","🔍 Processing 11: q_710 | sampling1 vs sampling0\n","✅ Saved: q_710_sampling1_vs_sampling0_error_fix_analysis.png\n","🔍 Processing 12: q_711 | sampling0 vs sampling1\n","✅ Saved: q_711_sampling0_vs_sampling1_error_fix_analysis.png\n","🔍 Processing 13: q_712 | sampling0 vs sampling2\n","✅ Saved: q_712_sampling0_vs_sampling2_error_fix_analysis.png\n","🔍 Processing 14: q_712 | sampling1 vs sampling2\n","✅ Saved: q_712_sampling1_vs_sampling2_error_fix_analysis.png\n","🔍 Processing 15: q_714 | sampling0 vs sampling2\n","✅ Saved: q_714_sampling0_vs_sampling2_error_fix_analysis.png\n","📊 Processed 15/15 samples\n","🔍 Processing 16: q_714 | sampling1 vs sampling2\n","✅ Saved: q_714_sampling1_vs_sampling2_error_fix_analysis.png\n","🔍 Processing 17: q_715 | sampling1 vs sampling0\n","✅ Saved: q_715_sampling1_vs_sampling0_error_fix_analysis.png\n","🔍 Processing 18: q_717 | sampling1 vs sampling0\n","✅ Saved: q_717_sampling1_vs_sampling0_error_fix_analysis.png\n","🔍 Processing 19: q_718 | sampling2 vs sampling0\n","✅ Saved: q_718_sampling2_vs_sampling0_error_fix_analysis.png\n","🔍 Processing 20: q_720 | sampling1 vs sampling0\n","✅ Saved: q_720_sampling1_vs_sampling0_error_fix_analysis.png\n","📊 Processed 20/20 samples\n","🔍 Processing 21: q_721 | sampling2 vs sampling0\n","✅ Saved: q_721_sampling2_vs_sampling0_error_fix_analysis.png\n","🔍 Processing 22: q_723 | sampling2 vs sampling0\n","✅ Saved: q_723_sampling2_vs_sampling0_error_fix_analysis.png\n","🔍 Processing 23: q_724 | sampling0 vs sampling1\n","✅ Saved: q_724_sampling0_vs_sampling1_error_fix_analysis.png\n","🔍 Processing 24: q_724 | sampling2 vs sampling1\n","✅ Saved: q_724_sampling2_vs_sampling1_error_fix_analysis.png\n","🔍 Processing 25: q_727 | sampling0 vs sampling2\n","✅ Saved: q_727_sampling0_vs_sampling2_error_fix_analysis.png\n","📊 Processed 25/25 samples\n","🔍 Processing 26: q_727 | sampling1 vs sampling2\n","✅ Saved: q_727_sampling1_vs_sampling2_error_fix_analysis.png\n","🔍 Processing 27: q_730 | sampling2 vs sampling0\n","✅ Saved: q_730_sampling2_vs_sampling0_error_fix_analysis.png\n","\n","🎉 Analysis completed!\n","📊 Total samples: 27\n","✅ Successfully processed: 27\n","❌ Failed to process: 0\n","📁 Main analysis plots saved to: /content/drive/MyDrive/Cluster-proj/output/fourier_analysis_error_fix\n","📁 Detailed analysis plots saved to: /content/drive/MyDrive/Cluster-proj/output/fourier_analysis_detail\n","📄 Processing summary saved to: /content/drive/MyDrive/Cluster-proj/output/fourier_analysis_error_fix/processing_summary.json\n"]}]}]}
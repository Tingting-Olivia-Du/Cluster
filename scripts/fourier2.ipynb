{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMJKVjyXzeIp1vuuOLjyFla"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"XFafq1fspDLm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752394397110,"user_tz":-480,"elapsed":2837,"user":{"displayName":"Tingting Du","userId":"01262363838823204487"}},"outputId":"b1cebd34-79fd-4560-e9a7-2fdacadc55e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Fourier analysis comparing positive/negative samples with error and fix positions\n","\"\"\"\n","\n","import os\n","import json\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.fft import fft, fftfreq, ifft\n","from google.colab import drive\n","\n","# âœ… æŒ‚è½½ Google Drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Memory-efficient Fourier analysis using streaming processing\n","\"\"\"\n","\n","import os\n","import json\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.fft import fft, fftfreq, ifft\n","import gc\n","from google.colab import drive\n","\n","# âœ… æŒ‚è½½ Google Drive\n","# drive.mount('/content/drive')\n","\n","# âœ… é…ç½®è·¯å¾„å‚æ•°\n","start_index = 700\n","end_index = 731\n","range_tag = f\"{start_index}-{end_index}\"\n","BASE_PATH = \"/content/drive/MyDrive/Cluster-proj\"\n","\n","# è¾“å…¥æ–‡ä»¶è·¯å¾„\n","LOGITS_JSONL_PATH = f\"{BASE_PATH}/output/llm_steps/whole_logits/deepseek7b-gsm-{range_tag}.jsonl\"\n","ERROR_FIX_INDEX_PATH = f\"{BASE_PATH}/output/error_fix_index/deepseek-7b-{range_tag}_error_fix_index.json\"\n","\n","# âœ… æ£€æŸ¥JSONLæ–‡ä»¶æ˜¯å¦å­˜åœ¨\n","if not os.path.exists(LOGITS_JSONL_PATH):\n","    print(f\"âš ï¸ JSONL file not found: {LOGITS_JSONL_PATH}\")\n","    print(\"Converting JSON to JSONL first...\")\n","\n","    # å¿«é€Ÿè½¬æ¢JSONåˆ°JSONL\n","    json_path = f\"{BASE_PATH}/output/llm_steps/whole_logits/deepseek-math-7b-gsm-{range_tag}.json\"\n","\n","    def convert_json_to_jsonl_chunked(json_path, jsonl_path, chunk_size=10):\n","        \"\"\"åˆ†å—è½¬æ¢JSONåˆ°JSONLä»¥èŠ‚çœå†…å­˜\"\"\"\n","        print(f\"ğŸ”„ Converting {json_path} to JSONL...\")\n","\n","        with open(json_path, 'r', encoding='utf-8') as f:\n","            data = json.load(f)\n","\n","        total_items = len(data)\n","        print(f\"ğŸ“Š Total items: {total_items}\")\n","\n","        with open(jsonl_path, 'w', encoding='utf-8') as f:\n","            for i, (qid, sample) in enumerate(data.items()):\n","                line_data = {\"qid\": qid, \"data\": sample}\n","                f.write(json.dumps(line_data, ensure_ascii=False) + '\\n')\n","\n","                if (i + 1) % chunk_size == 0:\n","                    print(f\"ğŸ“ˆ Converted {i + 1}/{total_items}\")\n","                    f.flush()  # å¼ºåˆ¶å†™å…¥ç£ç›˜\n","\n","        # æ¸…ç†å†…å­˜\n","        del data\n","        gc.collect()\n","        print(f\"âœ… Conversion complete: {jsonl_path}\")\n","\n","    convert_json_to_jsonl_chunked(json_path, LOGITS_JSONL_PATH)\n","\n","# âœ… åŠ è½½é”™è¯¯ä¿®å¤ç´¢å¼•æ•°æ®\n","print(\"ğŸ“‚ Loading error-fix index data...\")\n","if os.path.exists(ERROR_FIX_INDEX_PATH):\n","    with open(ERROR_FIX_INDEX_PATH, \"r\") as f:\n","        error_fix_data = json.load(f)\n","    print(f\"âœ… Loaded error-fix data: {len(error_fix_data)} questions\")\n","else:\n","    print(f\"âš ï¸ Error-fix index file not found: {ERROR_FIX_INDEX_PATH}\")\n","    print(\"Please run the error analysis script first!\")\n","    exit()\n","\n","# âœ… è¾“å‡ºè·¯å¾„\n","OUTPUT_DIR = f\"{BASE_PATH}/output/fourier_analysis_error_fix\"\n","DETAIL_DIR = f\"{BASE_PATH}/output/fourier_analysis_detail\"\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","os.makedirs(DETAIL_DIR, exist_ok=True)\n","\n","# âœ… å‚…é‡Œå¶å¹³æ»‘å‡½æ•°\n","def fourier_smooth(y, keep_ratio=0.1):\n","    \"\"\"å‚…é‡Œå¶å˜æ¢å¹³æ»‘å‡½æ•°\"\"\"\n","    y = np.asarray(y)\n","    N = len(y)\n","    if N < 4:\n","        return y\n","    Y = fft(y)\n","    Y[int(N * keep_ratio):-int(N * keep_ratio)] = 0\n","    y_smooth = np.real(ifft(Y))\n","    return y_smooth\n","\n","# âœ… æµå¼åŠ è½½å•ä¸ªæ ·æœ¬çš„logitsæ•°æ®\n","def load_sample_logits(qid, sampling_ids):\n","    \"\"\"\n","    ä»JSONLæ–‡ä»¶ä¸­æµå¼åŠ è½½æŒ‡å®šæ ·æœ¬çš„logitsæ•°æ®\n","    \"\"\"\n","    sample_data = {}\n","\n","    with open(LOGITS_JSONL_PATH, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            line_data = json.loads(line.strip())\n","            if line_data[\"qid\"] == qid:\n","                data = line_data[\"data\"]\n","                # åªæå–éœ€è¦çš„samplingæ•°æ®\n","                for sid in sampling_ids:\n","                    if sid in data:\n","                        sample_data[sid] = data[sid]\n","                break\n","\n","    return sample_data\n","\n","# âœ… ç»˜åˆ¶ç†µæ›²çº¿å¯¹æ¯”ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰\n","def plot_entropy_with_error_fix_positions_efficient(qid, neg_sid, pos_sid, analysis_data, save_dir):\n","    \"\"\"\n","    å†…å­˜æ•ˆç‡ä¼˜åŒ–çš„ç†µæ›²çº¿ç»˜åˆ¶å‡½æ•°\n","    \"\"\"\n","    try:\n","        # æµå¼åŠ è½½æ‰€éœ€çš„æ ·æœ¬æ•°æ®\n","        sample_logits = load_sample_logits(qid, [neg_sid, pos_sid])\n","\n","        if neg_sid not in sample_logits or pos_sid not in sample_logits:\n","            print(f\"âš ï¸ ç¼ºå¤± logitsï¼š{qid} | {neg_sid} / {pos_sid}\")\n","            return\n","\n","        neg_probs = sample_logits[neg_sid][\"token_probs\"]\n","        pos_probs = sample_logits[pos_sid][\"token_probs\"]\n","\n","        # æå–ç†µå€¼\n","        entropy_neg = np.array([tok[\"topk_info\"][\"entropy\"] for tok in neg_probs])\n","        entropy_pos = np.array([tok[\"topk_info\"][\"entropy\"] for tok in pos_probs])\n","\n","        if len(entropy_neg) < 4 or len(entropy_pos) < 4:\n","            print(f\"âš ï¸ åºåˆ—å¤ªçŸ­ï¼Œè·³è¿‡ {qid}\")\n","            return\n","\n","        idx_neg = np.arange(len(entropy_neg))\n","        idx_pos = np.arange(len(entropy_pos))\n","\n","        # å‚…é‡Œå¶å¹³æ»‘\n","        smooth_neg = fourier_smooth(entropy_neg, keep_ratio=0.1)\n","        smooth_pos = fourier_smooth(entropy_pos, keep_ratio=0.1)\n","\n","        # é”™è¯¯å’Œä¿®å¤ä½ç½®\n","        error_begin = analysis_data.get(\"error_token_begin_index\", -1)\n","        error_end = analysis_data.get(\"error_token_end_index\", -1)\n","        fix_begin = analysis_data.get(\"fix_token_begin_index\", -1)\n","        fix_end = analysis_data.get(\"fix_token_end_index\", -1)\n","\n","        # âœ… åˆ›å»ºå›¾å½¢\n","        fig, axs = plt.subplots(2, 2, figsize=(16, 10))\n","\n","        # å­å›¾1: è´Ÿæ ·æœ¬ï¼ˆé”™è¯¯ï¼‰çš„ç†µæ›²çº¿\n","        axs[0, 0].plot(idx_neg, entropy_neg, 'o-', alpha=0.3, label=\"Original Entropy\", color='red')\n","        axs[0, 0].plot(idx_neg, smooth_neg, '-', label=\"Smoothed Entropy\", color='red', linewidth=2)\n","\n","        # æ ‡æ³¨é”™è¯¯ä½ç½®\n","        if error_begin >= 0 and error_end >= 0:\n","            axs[0, 0].axvspan(error_begin, error_end, alpha=0.2, color='red', label=\"Error Region\")\n","            axs[0, 0].axvline(error_begin, color='red', linestyle='--', alpha=0.7)\n","            axs[0, 0].axvline(error_end, color='red', linestyle='--', alpha=0.7)\n","\n","        axs[0, 0].set_title(f\"Negative Sample ({neg_sid}) - Error Position\")\n","        axs[0, 0].set_xlabel(\"Token Index\")\n","        axs[0, 0].set_ylabel(\"Entropy\")\n","        axs[0, 0].legend()\n","        axs[0, 0].grid(True)\n","\n","        # å­å›¾2: æ­£æ ·æœ¬ï¼ˆæ­£ç¡®ï¼‰çš„ç†µæ›²çº¿\n","        axs[0, 1].plot(idx_pos, entropy_pos, 'o-', alpha=0.3, label=\"Original Entropy\", color='green')\n","        axs[0, 1].plot(idx_pos, smooth_pos, '-', label=\"Smoothed Entropy\", color='green', linewidth=2)\n","\n","        # æ ‡æ³¨ä¿®å¤ä½ç½®\n","        if fix_begin >= 0 and fix_end >= 0:\n","            axs[0, 1].axvspan(fix_begin, fix_end, alpha=0.2, color='blue', label=\"Fix Region\")\n","            axs[0, 1].axvline(fix_begin, color='blue', linestyle='--', alpha=0.7)\n","            axs[0, 1].axvline(fix_end, color='blue', linestyle='--', alpha=0.7)\n","\n","        axs[0, 1].set_title(f\"Positive Sample ({pos_sid}) - Fix Position\")\n","        axs[0, 1].set_xlabel(\"Token Index\")\n","        axs[0, 1].set_ylabel(\"Entropy\")\n","        axs[0, 1].legend()\n","        axs[0, 1].grid(True)\n","\n","        # å­å›¾3: å¯¹æ¯”å›¾ï¼ˆé‡å æ˜¾ç¤ºï¼‰\n","        axs[1, 0].plot(idx_neg, smooth_neg, '-', label=f\"{neg_sid} (Error)\", color='red', linewidth=2)\n","        axs[1, 0].plot(idx_pos, smooth_pos, '-', label=f\"{pos_sid} (Correct)\", color='green', linewidth=2)\n","\n","        # åŒæ—¶æ ‡æ³¨é”™è¯¯å’Œä¿®å¤ä½ç½®\n","        if error_begin >= 0 and error_end >= 0:\n","            axs[1, 0].axvspan(error_begin, error_end, alpha=0.15, color='red', label=\"Error Region\")\n","        if fix_begin >= 0 and fix_end >= 0:\n","            axs[1, 0].axvspan(fix_begin, fix_end, alpha=0.15, color='blue', label=\"Fix Region\")\n","\n","        axs[1, 0].set_title(\"Positive vs Negative Sample Comparison\")\n","        axs[1, 0].set_xlabel(\"Token Index\")\n","        axs[1, 0].set_ylabel(\"Entropy\")\n","        axs[1, 0].legend()\n","        axs[1, 0].grid(True)\n","\n","        # å­å›¾4: é¢‘è°±åˆ†æ\n","        N_neg = len(entropy_neg)\n","        N_pos = len(entropy_pos)\n","\n","        freqs_neg = fftfreq(N_neg, d=1)[:N_neg // 2]\n","        amp_neg = np.abs(fft(entropy_neg))[:N_neg // 2]\n","\n","        freqs_pos = fftfreq(N_pos, d=1)[:N_pos // 2]\n","        amp_pos = np.abs(fft(entropy_pos))[:N_pos // 2]\n","\n","        axs[1, 1].plot(freqs_neg, amp_neg, label=f\"{neg_sid} Spectrum\", color='red')\n","        axs[1, 1].plot(freqs_pos, amp_pos, label=f\"{pos_sid} Spectrum\", color='green')\n","        axs[1, 1].set_title(\"Fourier Amplitude Spectrum Comparison\")\n","        axs[1, 1].set_xlabel(\"Frequency\")\n","        axs[1, 1].set_ylabel(\"Amplitude\")\n","        axs[1, 1].legend()\n","        axs[1, 1].grid(True)\n","\n","        # è®¾ç½®æ•´ä½“æ ‡é¢˜\n","        plt.suptitle(f\"Entropy Analysis: {qid} | {neg_sid} vs {pos_sid}\", fontsize=14)\n","        plt.tight_layout()\n","\n","        # ä¿å­˜å›¾ç‰‡\n","        fname = f\"{qid}_{neg_sid}_vs_{pos_sid}_error_fix_analysis.png\"\n","        plt.savefig(os.path.join(save_dir, fname), dpi=150, bbox_inches='tight')\n","        plt.close()\n","\n","        # æ¸…ç†å†…å­˜\n","        del sample_logits, neg_probs, pos_probs, entropy_neg, entropy_pos\n","        gc.collect()\n","\n","        print(f\"âœ… Saved: {fname}\")\n","\n","    except Exception as e:\n","        print(f\"âŒ Processing failed {qid} | {neg_sid} vs {pos_sid}: {e}\")\n","\n","# âœ… ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Šå›¾ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰\n","def plot_detailed_position_analysis_efficient(qid, neg_sid, pos_sid, analysis_data, save_dir):\n","    \"\"\"\n","    å†…å­˜æ•ˆç‡ä¼˜åŒ–çš„è¯¦ç»†ä½ç½®åˆ†æå›¾\n","    \"\"\"\n","    try:\n","        # æµå¼åŠ è½½æ‰€éœ€çš„æ ·æœ¬æ•°æ®\n","        sample_logits = load_sample_logits(qid, [neg_sid, pos_sid])\n","\n","        if neg_sid not in sample_logits or pos_sid not in sample_logits:\n","            return\n","\n","        neg_probs = sample_logits[neg_sid][\"token_probs\"]\n","        pos_probs = sample_logits[pos_sid][\"token_probs\"]\n","\n","        entropy_neg = np.array([tok[\"topk_info\"][\"entropy\"] for tok in neg_probs])\n","        entropy_pos = np.array([tok[\"topk_info\"][\"entropy\"] for tok in pos_probs])\n","\n","        # æå–é”™è¯¯å’Œä¿®å¤åŒºé—´çš„ç†µå€¼\n","        error_begin = analysis_data.get(\"error_token_begin_index\", -1)\n","        error_end = analysis_data.get(\"error_token_end_index\", -1)\n","        fix_begin = analysis_data.get(\"fix_token_begin_index\", -1)\n","        fix_end = analysis_data.get(\"fix_token_end_index\", -1)\n","\n","        fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n","\n","        # å­å›¾1: é”™è¯¯åŒºé—´è¯¦ç»†åˆ†æ\n","        if error_begin >= 0 and error_end >= 0 and error_end < len(entropy_neg):\n","            error_entropy = entropy_neg[error_begin:error_end+1]\n","            error_indices = np.arange(error_begin, error_end+1)\n","\n","            axs[0].plot(error_indices, error_entropy, 'ro-', linewidth=2, markersize=6)\n","            axs[0].set_title(\"Error Region Entropy Details\")\n","            axs[0].set_xlabel(\"Token Index\")\n","            axs[0].set_ylabel(\"Entropy\")\n","            axs[0].grid(True)\n","\n","            # æ·»åŠ tokenä¿¡æ¯ï¼ˆé™åˆ¶æ•°é‡ä»¥é¿å…æ‹¥æŒ¤ï¼‰\n","            max_annotations = min(10, len(error_indices))\n","            step = max(1, len(error_indices) // max_annotations)\n","            for i in range(0, len(error_indices), step):\n","                idx = error_indices[i]\n","                if idx < len(neg_probs):\n","                    token_text = neg_probs[idx][\"token\"][:10]  # é™åˆ¶é•¿åº¦\n","                    axs[0].annotate(f\"{token_text}\", (idx, error_entropy[i]),\n","                                  textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=8)\n","\n","        # å­å›¾2: ä¿®å¤åŒºé—´è¯¦ç»†åˆ†æ\n","        if fix_begin >= 0 and fix_end >= 0 and fix_end < len(entropy_pos):\n","            fix_entropy = entropy_pos[fix_begin:fix_end+1]\n","            fix_indices = np.arange(fix_begin, fix_end+1)\n","\n","            axs[1].plot(fix_indices, fix_entropy, 'go-', linewidth=2, markersize=6)\n","            axs[1].set_title(\"Fix Region Entropy Details\")\n","            axs[1].set_xlabel(\"Token Index\")\n","            axs[1].set_ylabel(\"Entropy\")\n","            axs[1].grid(True)\n","\n","            # æ·»åŠ tokenä¿¡æ¯\n","            max_annotations = min(10, len(fix_indices))\n","            step = max(1, len(fix_indices) // max_annotations)\n","            for i in range(0, len(fix_indices), step):\n","                idx = fix_indices[i]\n","                if idx < len(pos_probs):\n","                    token_text = pos_probs[idx][\"token\"][:10]  # é™åˆ¶é•¿åº¦\n","                    axs[1].annotate(f\"{token_text}\", (idx, fix_entropy[i]),\n","                                  textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=8)\n","\n","        # å­å›¾3: é”™è¯¯vsä¿®å¤åŒºé—´å¯¹æ¯”\n","        if (error_begin >= 0 and error_end >= 0 and error_end < len(entropy_neg) and\n","            fix_begin >= 0 and fix_end >= 0 and fix_end < len(entropy_pos)):\n","\n","            error_entropy = entropy_neg[error_begin:error_end+1]\n","            fix_entropy = entropy_pos[fix_begin:fix_end+1]\n","\n","            # å¯¹é½é•¿åº¦è¿›è¡Œå¯¹æ¯”\n","            min_len = min(len(error_entropy), len(fix_entropy))\n","            if min_len > 0:\n","                x_indices = np.arange(min_len)\n","                axs[2].plot(x_indices, error_entropy[:min_len], 'ro-', label=\"Error Region\", linewidth=2)\n","                axs[2].plot(x_indices, fix_entropy[:min_len], 'go-', label=\"Fix Region\", linewidth=2)\n","                axs[2].set_title(\"Error vs Fix Region Comparison\")\n","                axs[2].set_xlabel(\"Relative Position\")\n","                axs[2].set_ylabel(\"Entropy\")\n","                axs[2].legend()\n","                axs[2].grid(True)\n","\n","        plt.suptitle(f\"Position Analysis: {qid} | Error vs Fix\", fontsize=14)\n","        plt.tight_layout()\n","\n","        fname = f\"{qid}_{neg_sid}_vs_{pos_sid}_position_detail.png\"\n","        plt.savefig(os.path.join(save_dir, fname), dpi=150, bbox_inches='tight')\n","        plt.close()\n","\n","        # æ¸…ç†å†…å­˜\n","        del sample_logits, neg_probs, pos_probs, entropy_neg, entropy_pos\n","        gc.collect()\n","\n","    except Exception as e:\n","        print(f\"âŒ Detailed analysis failed {qid}: {e}\")\n","\n","# âœ… ä¸»å¤„ç†å¾ªç¯ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰\n","print(\"\\nğŸš€ Starting analysis generation...\")\n","\n","# ç»Ÿè®¡ä¿¡æ¯\n","total_samples = 0\n","processed_count = 0\n","error_count = 0\n","\n","# é€ä¸ªå¤„ç†æ ·æœ¬ï¼Œé¿å…å†…å­˜ç´¯ç§¯\n","for qid, sample_data in error_fix_data.items():\n","    for neg_sid, analysis in sample_data.items():\n","        pos_sid = analysis.get(\"correct_sampling_id\")\n","        if pos_sid:\n","            total_samples += 1\n","\n","            try:\n","                print(f\"ğŸ” Processing {total_samples}: {qid} | {neg_sid} vs {pos_sid}\")\n","\n","                # ç”Ÿæˆä¸»è¦åˆ†æå›¾\n","                plot_entropy_with_error_fix_positions_efficient(\n","                    qid, neg_sid, pos_sid, analysis, OUTPUT_DIR\n","                )\n","\n","                # ç”Ÿæˆè¯¦ç»†ä½ç½®åˆ†æå›¾\n","                plot_detailed_position_analysis_efficient(\n","                    qid, neg_sid, pos_sid, analysis, DETAIL_DIR\n","                )\n","\n","                processed_count += 1\n","\n","                # å®šæœŸæ¸…ç†å†…å­˜\n","                if total_samples % 5 == 0:\n","                    gc.collect()\n","                    print(f\"ğŸ“Š Processed {processed_count}/{total_samples} samples\")\n","\n","            except Exception as e:\n","                print(f\"âŒ Processing failed {qid} | {neg_sid} vs {pos_sid}: {e}\")\n","                error_count += 1\n","                continue\n","\n","# âœ… ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š\n","print(f\"\\nğŸ‰ Analysis completed!\")\n","print(f\"ğŸ“Š Total samples: {total_samples}\")\n","print(f\"âœ… Successfully processed: {processed_count}\")\n","print(f\"âŒ Failed to process: {error_count}\")\n","print(f\"ğŸ“ Main analysis plots saved to: {OUTPUT_DIR}\")\n","print(f\"ğŸ“ Detailed analysis plots saved to: {DETAIL_DIR}\")\n","\n","# âœ… ç”Ÿæˆå¤„ç†æ‘˜è¦\n","summary_data = {\n","    \"processing_summary\": {\n","        \"total_samples\": total_samples,\n","        \"successful_processed\": processed_count,\n","        \"failed_processed\": error_count,\n","        \"success_rate\": f\"{processed_count/total_samples*100:.1f}%\" if total_samples > 0 else \"0%\"\n","    },\n","    \"output_directories\": {\n","        \"main_analysis\": OUTPUT_DIR,\n","        \"detailed_analysis\": DETAIL_DIR\n","    }\n","}\n","\n","summary_file = os.path.join(OUTPUT_DIR, \"processing_summary.json\")\n","with open(summary_file, 'w', encoding='utf-8') as f:\n","    json.dump(summary_data, f, ensure_ascii=False, indent=2)\n","\n","print(f\"ğŸ“„ Processing summary saved to: {summary_file}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Hf8U7qOpVym","executionInfo":{"status":"ok","timestamp":1752395295646,"user_tz":-480,"elapsed":797098,"user":{"displayName":"Tingting Du","userId":"01262363838823204487"}},"outputId":"af6f3f68-88cf-41c9-80d9-a9ef0baa6bc9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ“‚ Loading error-fix index data...\n","âœ… Loaded error-fix data: 19 questions\n","\n","ğŸš€ Starting analysis generation...\n","ğŸ” Processing 1: q_700 | sampling0 vs sampling1\n","âœ… Saved: q_700_sampling0_vs_sampling1_error_fix_analysis.png\n","ğŸ” Processing 2: q_700 | sampling2 vs sampling1\n","âœ… Saved: q_700_sampling2_vs_sampling1_error_fix_analysis.png\n","ğŸ” Processing 3: q_701 | sampling1 vs sampling0\n","âœ… Saved: q_701_sampling1_vs_sampling0_error_fix_analysis.png\n","ğŸ” Processing 4: q_703 | sampling1 vs sampling0\n","âœ… Saved: q_703_sampling1_vs_sampling0_error_fix_analysis.png\n","ğŸ” Processing 5: q_705 | sampling1 vs sampling0\n","âœ… Saved: q_705_sampling1_vs_sampling0_error_fix_analysis.png\n","ğŸ“Š Processed 5/5 samples\n","ğŸ” Processing 6: q_705 | sampling2 vs sampling0\n","âœ… Saved: q_705_sampling2_vs_sampling0_error_fix_analysis.png\n","ğŸ” Processing 7: q_707 | sampling0 vs sampling1\n","âœ… Saved: q_707_sampling0_vs_sampling1_error_fix_analysis.png\n","ğŸ” Processing 8: q_707 | sampling2 vs sampling1\n","âœ… Saved: q_707_sampling2_vs_sampling1_error_fix_analysis.png\n","ğŸ” Processing 9: q_709 | sampling0 vs sampling1\n","âœ… Saved: q_709_sampling0_vs_sampling1_error_fix_analysis.png\n","ğŸ” Processing 10: q_709 | sampling2 vs sampling1\n","âœ… Saved: q_709_sampling2_vs_sampling1_error_fix_analysis.png\n","ğŸ“Š Processed 10/10 samples\n","ğŸ” Processing 11: q_710 | sampling1 vs sampling0\n","âœ… Saved: q_710_sampling1_vs_sampling0_error_fix_analysis.png\n","ğŸ” Processing 12: q_711 | sampling0 vs sampling1\n","âœ… Saved: q_711_sampling0_vs_sampling1_error_fix_analysis.png\n","ğŸ” Processing 13: q_712 | sampling0 vs sampling2\n","âœ… Saved: q_712_sampling0_vs_sampling2_error_fix_analysis.png\n","ğŸ” Processing 14: q_712 | sampling1 vs sampling2\n","âœ… Saved: q_712_sampling1_vs_sampling2_error_fix_analysis.png\n","ğŸ” Processing 15: q_714 | sampling0 vs sampling2\n","âœ… Saved: q_714_sampling0_vs_sampling2_error_fix_analysis.png\n","ğŸ“Š Processed 15/15 samples\n","ğŸ” Processing 16: q_714 | sampling1 vs sampling2\n","âœ… Saved: q_714_sampling1_vs_sampling2_error_fix_analysis.png\n","ğŸ” Processing 17: q_715 | sampling1 vs sampling0\n","âœ… Saved: q_715_sampling1_vs_sampling0_error_fix_analysis.png\n","ğŸ” Processing 18: q_717 | sampling1 vs sampling0\n","âœ… Saved: q_717_sampling1_vs_sampling0_error_fix_analysis.png\n","ğŸ” Processing 19: q_718 | sampling2 vs sampling0\n","âœ… Saved: q_718_sampling2_vs_sampling0_error_fix_analysis.png\n","ğŸ” Processing 20: q_720 | sampling1 vs sampling0\n","âœ… Saved: q_720_sampling1_vs_sampling0_error_fix_analysis.png\n","ğŸ“Š Processed 20/20 samples\n","ğŸ” Processing 21: q_721 | sampling2 vs sampling0\n","âœ… Saved: q_721_sampling2_vs_sampling0_error_fix_analysis.png\n","ğŸ” Processing 22: q_723 | sampling2 vs sampling0\n","âœ… Saved: q_723_sampling2_vs_sampling0_error_fix_analysis.png\n","ğŸ” Processing 23: q_724 | sampling0 vs sampling1\n","âœ… Saved: q_724_sampling0_vs_sampling1_error_fix_analysis.png\n","ğŸ” Processing 24: q_724 | sampling2 vs sampling1\n","âœ… Saved: q_724_sampling2_vs_sampling1_error_fix_analysis.png\n","ğŸ” Processing 25: q_727 | sampling0 vs sampling2\n","âœ… Saved: q_727_sampling0_vs_sampling2_error_fix_analysis.png\n","ğŸ“Š Processed 25/25 samples\n","ğŸ” Processing 26: q_727 | sampling1 vs sampling2\n","âœ… Saved: q_727_sampling1_vs_sampling2_error_fix_analysis.png\n","ğŸ” Processing 27: q_730 | sampling2 vs sampling0\n","âœ… Saved: q_730_sampling2_vs_sampling0_error_fix_analysis.png\n","\n","ğŸ‰ Analysis completed!\n","ğŸ“Š Total samples: 27\n","âœ… Successfully processed: 27\n","âŒ Failed to process: 0\n","ğŸ“ Main analysis plots saved to: /content/drive/MyDrive/Cluster-proj/output/fourier_analysis_error_fix\n","ğŸ“ Detailed analysis plots saved to: /content/drive/MyDrive/Cluster-proj/output/fourier_analysis_detail\n","ğŸ“„ Processing summary saved to: /content/drive/MyDrive/Cluster-proj/output/fourier_analysis_error_fix/processing_summary.json\n"]}]}]}
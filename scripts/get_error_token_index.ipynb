{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 192933,
     "status": "ok",
     "timestamp": 1752394079714,
     "user": {
      "displayName": "Tingting Du",
      "userId": "01262363838823204487"
     },
     "user_tz": -480
    },
    "id": "olcts10oX0nL",
    "outputId": "62b860f6-cd75-4ba6-bc60-56edc268efb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Converting ../logits/deepseek-math-7b-math-0-15-algebra-level_3.json to JSONL format...\n",
      "📊 Input file size: 1.05 GB\n",
      "📊 Total items to convert: 10\n",
      "✅ Conversion complete! Saved to ../logits/deepseek-math-7b-math-0-15-algebra-level_3.jsonl\n",
      "\n",
      "🚀 Starting JSONL processing...\n",
      "🔍 Processing q_0...\n",
      "⚠️ No valid result for q_0\n",
      "🔍 Processing q_1...\n",
      "  🔍 Analyzing sampling2...\n",
      "  ✅ Successfully analyzed sampling2\n",
      "✅ Successfully processed q_1\n",
      "🔍 Processing q_2...\n",
      "  🔍 Analyzing sampling0...\n",
      "  ✅ Successfully analyzed sampling0\n",
      "✅ Successfully processed q_2\n",
      "🔍 Processing q_3...\n",
      "⚠️ No valid result for q_3\n",
      "🔍 Processing q_4...\n",
      "⚠️ No valid result for q_4\n",
      "💾 Saved 2 results to ../error_fix_index/deepseek-math-7b-math-0-15-algebra-level_3_index.json\n",
      "📊 Progress: 2 processed, 0 errors, 0.23 items/sec, 8.7s elapsed\n",
      "🔍 Processing q_5...\n",
      "  🔍 Analyzing sampling0...\n",
      "  ✅ Successfully analyzed sampling0\n",
      "✅ Successfully processed q_5\n",
      "🔍 Processing q_6...\n",
      "⚠️ No valid result for q_6\n",
      "🔍 Processing q_7...\n",
      "⚠️ No valid result for q_7\n",
      "🔍 Processing q_8...\n",
      "⚠️ No valid result for q_8\n",
      "🔍 Processing q_9...\n",
      "⚠️ No valid result for q_9\n",
      "💾 Saved 3 results to ../error_fix_index/deepseek-math-7b-math-0-15-algebra-level_3_index.json\n",
      "📊 Progress: 3 processed, 0 errors, 0.16 items/sec, 19.2s elapsed\n",
      "💾 Saved 3 results to ../error_fix_index/deepseek-math-7b-math-0-15-algebra-level_3_index.json\n",
      "\n",
      "🎉 Processing complete!\n",
      "📊 Total processed: 3\n",
      "⚠️ Total errors: 0\n",
      "⏱️ Total time: 19.2s\n",
      "🚀 Average speed: 0.16 items/sec\n",
      "\n",
      "✅ All processing complete! Results saved to ../error_fix_index/deepseek-math-7b-math-0-15-algebra-level_3_index.json\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Complete JSONL processor with all required functions\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Iterator, Dict, Any\n",
    "\n",
    "# # ✅ API配置\n",
    "API_KEY = \"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\"\n",
    "MODEL = \"gpt-4.1\"\n",
    "\n",
    "# ✅ 构建错误分析提示词\n",
    "def build_error_prompt(question, true_whole_answer, sample_whole_answer):\n",
    "    \"\"\"构建用于错误分析的提示词\"\"\"\n",
    "    return f\"\"\"\n",
    "Here is a math question, its correct answer, and a sample answer that may contain mistakes.\n",
    "\n",
    "【question】:\n",
    "{question}\n",
    "\n",
    "【Correct Answer】:\n",
    "{true_whole_answer}\n",
    "\n",
    "【Incorrect Answer】:\n",
    "{sample_whole_answer}\n",
    "\n",
    "Please help me:\n",
    "1. Identify the earliest mistake in the incorrect answer and provide the complete sentence from that point.\n",
    "2. Briefly explain why it is incorrect.\n",
    "3. Find the fix sentence in correct answer that and fix the error.\n",
    "4. Briefly explain why it can fix the error.\n",
    "\n",
    "Please output in the following JSON format:\n",
    "{{\n",
    "  \"first_error_sentence\": \"<sentence>\",\n",
    "  \"error_reason\": \"<brief explanation>\",\n",
    "  \"fix_sentence\": \"<sentence>\",\n",
    "  \"fix_reason\": \"<brief explanation>\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# ✅ 调用GPT API\n",
    "def call_custom_gpt_api(prompt):\n",
    "    \"\"\"调用OpenAI API\"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a meticulous and precise comparer.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"https://api.openai.com/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "            timeout=30  # 添加超时\n",
    "        )\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"API request failed: {response.status_code}, {response.text}\")\n",
    "\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        raise Exception(\"API request timeout\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise Exception(f\"API request error: {str(e)}\")\n",
    "\n",
    "# ✅ 查找句子在token序列中的位置\n",
    "def find_sentence_span_indices_robust(fragment, token_probs):\n",
    "    \"\"\"\n",
    "    返回 fragment 在 token_probs 中匹配到的 token 范围: (begin_index, end_index)\n",
    "    使用去除空白字符的方式匹配\n",
    "    \"\"\"\n",
    "    if not fragment or not token_probs:\n",
    "        return -1, -1\n",
    "\n",
    "    fragment_clean = re.sub(r\"\\s+\", \"\", fragment)\n",
    "    tokens = [entry[\"token\"] for entry in token_probs]\n",
    "    decoded_text = \"\".join(tokens)\n",
    "    decoded_text_clean = re.sub(r\"\\s+\", \"\", decoded_text)\n",
    "\n",
    "    char_start_idx = decoded_text_clean.find(fragment_clean)\n",
    "    if char_start_idx == -1:\n",
    "        return -1, -1\n",
    "\n",
    "    cumulative_len = 0\n",
    "    begin_index = -1\n",
    "\n",
    "    for idx, entry in enumerate(token_probs):\n",
    "        token_clean = re.sub(r\"\\s+\", \"\", entry[\"token\"])\n",
    "        prev_len = cumulative_len\n",
    "        cumulative_len += len(token_clean)\n",
    "\n",
    "        if begin_index == -1 and cumulative_len > char_start_idx:\n",
    "            begin_index = idx\n",
    "        if cumulative_len >= char_start_idx + len(fragment_clean):\n",
    "            end_index = idx\n",
    "            return begin_index, end_index\n",
    "\n",
    "    return begin_index, len(token_probs) - 1  # fallback\n",
    "\n",
    "class JSONLProcessor:\n",
    "    \"\"\"\n",
    "    高效的JSONL处理器，支持内存管理和进度跟踪\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str, model: str = \"gpt-4.1\"):\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self.processed_count = 0\n",
    "        self.error_count = 0\n",
    "        self.start_time = None\n",
    "\n",
    "    def convert_json_to_jsonl(self, input_path: str, output_path: str,\n",
    "                             chunk_size: int = 1000):\n",
    "        \"\"\"\n",
    "        将大JSON文件转换为JSONL，支持分块处理\n",
    "        \"\"\"\n",
    "        print(f\"🔄 Converting {input_path} to JSONL format...\")\n",
    "\n",
    "        # 创建输出目录\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "        # 检查文件大小\n",
    "        file_size = os.path.getsize(input_path)\n",
    "        print(f\"📊 Input file size: {file_size / (1024**3):.2f} GB\")\n",
    "\n",
    "        with open(input_path, 'r', encoding='utf-8') as infile:\n",
    "            data = json.load(infile)\n",
    "\n",
    "        total_items = len(data)\n",
    "        print(f\"📊 Total items to convert: {total_items}\")\n",
    "\n",
    "        with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "            for i, (qid, sample) in enumerate(data.items()):\n",
    "                line_data = {\n",
    "                    \"qid\": qid,\n",
    "                    \"data\": sample\n",
    "                }\n",
    "                outfile.write(json.dumps(line_data, ensure_ascii=False) + '\\n')\n",
    "\n",
    "                if (i + 1) % chunk_size == 0:\n",
    "                    print(f\"📈 Converted {i + 1}/{total_items} items...\")\n",
    "                    # 强制刷新到磁盘\n",
    "                    outfile.flush()\n",
    "\n",
    "        print(f\"✅ Conversion complete! Saved to {output_path}\")\n",
    "\n",
    "        # 清理内存\n",
    "        del data\n",
    "        gc.collect()\n",
    "\n",
    "    def process_jsonl_file(self, jsonl_path: str, output_path: str,\n",
    "                          batch_size: int = 10, save_interval: int = 20):\n",
    "        \"\"\"\n",
    "        流式处理JSONL文件，支持批处理和定期保存\n",
    "        \"\"\"\n",
    "        self.start_time = time.time()\n",
    "        results = {}\n",
    "\n",
    "        # 如果输出文件已存在，加载已处理的结果\n",
    "        if os.path.exists(output_path):\n",
    "            print(\"📂 Loading existing results...\")\n",
    "            try:\n",
    "                with open(output_path, 'r', encoding='utf-8') as f:\n",
    "                    results = json.load(f)\n",
    "                    self.processed_count = len(results)\n",
    "                    print(f\"📊 Loaded {self.processed_count} existing results\")\n",
    "            except (json.JSONDecodeError, FileNotFoundError):\n",
    "                print(\"⚠️ Could not load existing results, starting fresh\")\n",
    "                results = {}\n",
    "\n",
    "        # 创建输出目录\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            batch = []\n",
    "            line_count = 0\n",
    "\n",
    "            for line in f:\n",
    "                try:\n",
    "                    line_data = json.loads(line.strip())\n",
    "                    qid = line_data[\"qid\"]\n",
    "\n",
    "                    # 跳过已处理的项目\n",
    "                    if qid in results:\n",
    "                        print(f\"⏭️ Skipping already processed: {qid}\")\n",
    "                        continue\n",
    "\n",
    "                    batch.append((qid, line_data[\"data\"]))\n",
    "                    line_count += 1\n",
    "\n",
    "                    # 处理批次\n",
    "                    if len(batch) >= batch_size:\n",
    "                        self._process_batch(batch, results)\n",
    "                        batch = []\n",
    "\n",
    "                        # 定期保存和清理内存\n",
    "                        if line_count % save_interval == 0:\n",
    "                            self._save_results(results, output_path)\n",
    "                            gc.collect()\n",
    "                            self._print_progress()\n",
    "\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"⚠️ JSON decode error in line: {e}\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Unexpected error processing line: {e}\")\n",
    "                    continue\n",
    "\n",
    "            # 处理剩余的项目\n",
    "            if batch:\n",
    "                self._process_batch(batch, results)\n",
    "\n",
    "        # 最终保存\n",
    "        self._save_results(results, output_path)\n",
    "        self._print_final_stats()\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _process_batch(self, batch: list, results: dict):\n",
    "        \"\"\"处理一个批次的数据\"\"\"\n",
    "        for qid, sample in batch:\n",
    "            try:\n",
    "                print(f\"🔍 Processing {qid}...\")\n",
    "                result = self._process_single_sample(qid, sample)\n",
    "                if result:\n",
    "                    results[qid] = result\n",
    "                    self.processed_count += 1\n",
    "                    print(f\"✅ Successfully processed {qid}\")\n",
    "                else:\n",
    "                    print(f\"⚠️ No valid result for {qid}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error processing {qid}: {str(e)}\")\n",
    "                self.error_count += 1\n",
    "                continue\n",
    "\n",
    "    def _process_single_sample(self, qid: str, sample: dict) -> dict:\n",
    "        \"\"\"处理单个样本\"\"\"\n",
    "        try:\n",
    "            question = sample.get(\"question\", \"\")\n",
    "            true_final_result = sample.get(\"true_final_result\", \"\")\n",
    "\n",
    "            if not question or not true_final_result:\n",
    "                print(f\"⚠️ Missing question or true_final_result for {qid}\")\n",
    "                return None\n",
    "\n",
    "            # 找到正样本\n",
    "            correct_sampling_id = None\n",
    "            correct_sample_answer = None\n",
    "\n",
    "            for sampling_id in [\"sampling0\", \"sampling1\", \"sampling2\"]:\n",
    "                if sampling_id not in sample:\n",
    "                    continue\n",
    "                sampling_data = sample[sampling_id]\n",
    "                if sampling_data.get(\"final_result\") == true_final_result:\n",
    "                    correct_sampling_id = sampling_id\n",
    "                    correct_sample_answer = sampling_data.get(\"whole_answer\", \"\")\n",
    "                    break\n",
    "\n",
    "            if correct_sample_answer is None:\n",
    "                print(f\"⚠️ No correct sampling found for {qid}\")\n",
    "                return None\n",
    "\n",
    "            sample_results = {}\n",
    "\n",
    "            # 处理负样本\n",
    "            for sampling_id in [\"sampling0\", \"sampling1\", \"sampling2\"]:\n",
    "                if sampling_id not in sample:\n",
    "                    continue\n",
    "                sampling = sample[sampling_id]\n",
    "\n",
    "                # 跳过正样本\n",
    "                if sampling.get(\"final_result\") == true_final_result:\n",
    "                    continue\n",
    "\n",
    "                incorrect_sample_answer = sampling.get(\"whole_answer\", \"\")\n",
    "                if not incorrect_sample_answer:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    print(f\"  🔍 Analyzing {sampling_id}...\")\n",
    "\n",
    "                    # 调用API\n",
    "                    prompt = build_error_prompt(question, correct_sample_answer, incorrect_sample_answer)\n",
    "                    output = call_custom_gpt_api(prompt)\n",
    "\n",
    "                    # 解析结果\n",
    "                    output = output.strip().strip(\"```\")\n",
    "                    if output.startswith(\"json\"):\n",
    "                        output = output[4:].strip()\n",
    "\n",
    "                    output_json = json.loads(output)\n",
    "\n",
    "                    # 查找token索引\n",
    "                    error_sentence = output_json.get(\"first_error_sentence\", \"\")\n",
    "                    fix_sentence = output_json.get(\"fix_sentence\", \"\")\n",
    "\n",
    "                    error_token_probs = sampling.get(\"token_probs\", [])\n",
    "                    correct_token_probs = sample[correct_sampling_id].get(\"token_probs\", [])\n",
    "\n",
    "                    error_begin_idx, error_end_idx = find_sentence_span_indices_robust(\n",
    "                        error_sentence, error_token_probs\n",
    "                    )\n",
    "                    fix_begin_idx, fix_end_idx = find_sentence_span_indices_robust(\n",
    "                        fix_sentence, correct_token_probs\n",
    "                    )\n",
    "\n",
    "                    sample_results[sampling_id] = {\n",
    "                        \"first_error_sentence\": error_sentence,\n",
    "                        \"error_reason\": output_json.get(\"error_reason\", \"\"),\n",
    "                        \"fix_sentence\": fix_sentence,\n",
    "                        \"fix_reason\": output_json.get(\"fix_reason\", \"\"),\n",
    "                        \"correct_sampling_id\": correct_sampling_id,\n",
    "                        \"error_token_begin_index\": error_begin_idx,\n",
    "                        \"error_token_end_index\": error_end_idx,\n",
    "                        \"fix_token_begin_index\": fix_begin_idx,\n",
    "                        \"fix_token_end_index\": fix_end_idx\n",
    "                    }\n",
    "\n",
    "                    print(f\"  ✅ Successfully analyzed {sampling_id}\")\n",
    "\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"  ⚠️ JSON decode error for {sampling_id}: {e}\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"  ⚠️ Error analyzing {sampling_id}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            return sample_results if sample_results else None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error in _process_single_sample for {qid}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _save_results(self, results: dict, output_path: str):\n",
    "        \"\"\"保存结果到文件\"\"\"\n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"💾 Saved {len(results)} results to {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error saving results: {e}\")\n",
    "\n",
    "    def _print_progress(self):\n",
    "        \"\"\"打印进度信息\"\"\"\n",
    "        elapsed = time.time() - self.start_time\n",
    "        speed = self.processed_count / elapsed if elapsed > 0 else 0\n",
    "        print(f\"📊 Progress: {self.processed_count} processed, {self.error_count} errors, \"\n",
    "              f\"{speed:.2f} items/sec, {elapsed:.1f}s elapsed\")\n",
    "\n",
    "    def _print_final_stats(self):\n",
    "        \"\"\"打印最终统计信息\"\"\"\n",
    "        elapsed = time.time() - self.start_time\n",
    "        print(f\"\\n🎉 Processing complete!\")\n",
    "        print(f\"📊 Total processed: {self.processed_count}\")\n",
    "        print(f\"⚠️ Total errors: {self.error_count}\")\n",
    "        print(f\"⏱️ Total time: {elapsed:.1f}s\")\n",
    "        if elapsed > 0:\n",
    "            print(f\"🚀 Average speed: {self.processed_count / elapsed:.2f} items/sec\")\n",
    "\n",
    "# ✅ 主函数\n",
    "def main():\n",
    "    # 路径配置\n",
    "    \n",
    "    range_tag = \"0-15\"\n",
    "    level = \"level_3\"\n",
    "    discipline = \"algebra\"\n",
    "    input_json = f\"../logits/deepseek-math-7b-math-{range_tag}-{discipline}-{level}.json\"\n",
    "    output_jsonl = f\"../logits/deepseek-math-7b-math-{range_tag}-{discipline}-{level}.jsonl\"\n",
    "    output_results = f\"../error_fix_index/deepseek-math-7b-math-{range_tag}-{discipline}-{level}_index.json\"\n",
    "\n",
    "    # 创建处理器\n",
    "    processor = JSONLProcessor(API_KEY, MODEL)\n",
    "\n",
    "    # 步骤1: 转换为JSONL（如果还没有转换）\n",
    "    if not os.path.exists(output_jsonl):\n",
    "        processor.convert_json_to_jsonl(input_json, output_jsonl)\n",
    "    else:\n",
    "        print(f\"📂 JSONL file already exists: {output_jsonl}\")\n",
    "\n",
    "    # 步骤2: 处理JSONL文件\n",
    "    print(\"\\n🚀 Starting JSONL processing...\")\n",
    "    results = processor.process_jsonl_file(\n",
    "        jsonl_path=output_jsonl,\n",
    "        output_path=output_results,\n",
    "        batch_size=1,      # 设置为1以便调试\n",
    "        save_interval=5    # 每5个项目保存一次\n",
    "    )\n",
    "\n",
    "    print(f\"\\n✅ All processing complete! Results saved to {output_results}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMQPNedmW8IUavg3TIqI+SP",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cluster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

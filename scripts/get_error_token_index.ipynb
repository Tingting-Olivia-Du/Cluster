{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 192933,
     "status": "ok",
     "timestamp": 1752394079714,
     "user": {
      "displayName": "Tingting Du",
      "userId": "01262363838823204487"
     },
     "user_tz": -480
    },
    "id": "olcts10oX0nL",
    "outputId": "62b860f6-cd75-4ba6-bc60-56edc268efb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Converting ../logits/deepseek-math-7b-math-0-15-algebra-level_3.json to JSONL format...\n",
      "ğŸ“Š Input file size: 1.05 GB\n",
      "ğŸ“Š Total items to convert: 10\n",
      "âœ… Conversion complete! Saved to ../logits/deepseek-math-7b-math-0-15-algebra-level_3.jsonl\n",
      "\n",
      "ğŸš€ Starting JSONL processing...\n",
      "ğŸ” Processing q_0...\n",
      "âš ï¸ No valid result for q_0\n",
      "ğŸ” Processing q_1...\n",
      "  ğŸ” Analyzing sampling2...\n",
      "  âœ… Successfully analyzed sampling2\n",
      "âœ… Successfully processed q_1\n",
      "ğŸ” Processing q_2...\n",
      "  ğŸ” Analyzing sampling0...\n",
      "  âœ… Successfully analyzed sampling0\n",
      "âœ… Successfully processed q_2\n",
      "ğŸ” Processing q_3...\n",
      "âš ï¸ No valid result for q_3\n",
      "ğŸ” Processing q_4...\n",
      "âš ï¸ No valid result for q_4\n",
      "ğŸ’¾ Saved 2 results to ../error_fix_index/deepseek-math-7b-math-0-15-algebra-level_3_index.json\n",
      "ğŸ“Š Progress: 2 processed, 0 errors, 0.23 items/sec, 8.7s elapsed\n",
      "ğŸ” Processing q_5...\n",
      "  ğŸ” Analyzing sampling0...\n",
      "  âœ… Successfully analyzed sampling0\n",
      "âœ… Successfully processed q_5\n",
      "ğŸ” Processing q_6...\n",
      "âš ï¸ No valid result for q_6\n",
      "ğŸ” Processing q_7...\n",
      "âš ï¸ No valid result for q_7\n",
      "ğŸ” Processing q_8...\n",
      "âš ï¸ No valid result for q_8\n",
      "ğŸ” Processing q_9...\n",
      "âš ï¸ No valid result for q_9\n",
      "ğŸ’¾ Saved 3 results to ../error_fix_index/deepseek-math-7b-math-0-15-algebra-level_3_index.json\n",
      "ğŸ“Š Progress: 3 processed, 0 errors, 0.16 items/sec, 19.2s elapsed\n",
      "ğŸ’¾ Saved 3 results to ../error_fix_index/deepseek-math-7b-math-0-15-algebra-level_3_index.json\n",
      "\n",
      "ğŸ‰ Processing complete!\n",
      "ğŸ“Š Total processed: 3\n",
      "âš ï¸ Total errors: 0\n",
      "â±ï¸ Total time: 19.2s\n",
      "ğŸš€ Average speed: 0.16 items/sec\n",
      "\n",
      "âœ… All processing complete! Results saved to ../error_fix_index/deepseek-math-7b-math-0-15-algebra-level_3_index.json\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Complete JSONL processor with all required functions\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Iterator, Dict, Any\n",
    "\n",
    "# # âœ… APIé…ç½®\n",
    "API_KEY = \"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\"\n",
    "MODEL = \"gpt-4.1\"\n",
    "\n",
    "# âœ… æ„å»ºé”™è¯¯åˆ†ææç¤ºè¯\n",
    "def build_error_prompt(question, true_whole_answer, sample_whole_answer):\n",
    "    \"\"\"æ„å»ºç”¨äºé”™è¯¯åˆ†æçš„æç¤ºè¯\"\"\"\n",
    "    return f\"\"\"\n",
    "Here is a math question, its correct answer, and a sample answer that may contain mistakes.\n",
    "\n",
    "ã€questionã€‘:\n",
    "{question}\n",
    "\n",
    "ã€Correct Answerã€‘:\n",
    "{true_whole_answer}\n",
    "\n",
    "ã€Incorrect Answerã€‘:\n",
    "{sample_whole_answer}\n",
    "\n",
    "Please help me:\n",
    "1. Identify the earliest mistake in the incorrect answer and provide the complete sentence from that point.\n",
    "2. Briefly explain why it is incorrect.\n",
    "3. Find the fix sentence in correct answer that and fix the error.\n",
    "4. Briefly explain why it can fix the error.\n",
    "\n",
    "Please output in the following JSON format:\n",
    "{{\n",
    "  \"first_error_sentence\": \"<sentence>\",\n",
    "  \"error_reason\": \"<brief explanation>\",\n",
    "  \"fix_sentence\": \"<sentence>\",\n",
    "  \"fix_reason\": \"<brief explanation>\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# âœ… è°ƒç”¨GPT API\n",
    "def call_custom_gpt_api(prompt):\n",
    "    \"\"\"è°ƒç”¨OpenAI API\"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a meticulous and precise comparer.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"https://api.openai.com/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "            timeout=30  # æ·»åŠ è¶…æ—¶\n",
    "        )\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"API request failed: {response.status_code}, {response.text}\")\n",
    "\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        raise Exception(\"API request timeout\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise Exception(f\"API request error: {str(e)}\")\n",
    "\n",
    "# âœ… æŸ¥æ‰¾å¥å­åœ¨tokenåºåˆ—ä¸­çš„ä½ç½®\n",
    "def find_sentence_span_indices_robust(fragment, token_probs):\n",
    "    \"\"\"\n",
    "    è¿”å› fragment åœ¨ token_probs ä¸­åŒ¹é…åˆ°çš„ token èŒƒå›´: (begin_index, end_index)\n",
    "    ä½¿ç”¨å»é™¤ç©ºç™½å­—ç¬¦çš„æ–¹å¼åŒ¹é…\n",
    "    \"\"\"\n",
    "    if not fragment or not token_probs:\n",
    "        return -1, -1\n",
    "\n",
    "    fragment_clean = re.sub(r\"\\s+\", \"\", fragment)\n",
    "    tokens = [entry[\"token\"] for entry in token_probs]\n",
    "    decoded_text = \"\".join(tokens)\n",
    "    decoded_text_clean = re.sub(r\"\\s+\", \"\", decoded_text)\n",
    "\n",
    "    char_start_idx = decoded_text_clean.find(fragment_clean)\n",
    "    if char_start_idx == -1:\n",
    "        return -1, -1\n",
    "\n",
    "    cumulative_len = 0\n",
    "    begin_index = -1\n",
    "\n",
    "    for idx, entry in enumerate(token_probs):\n",
    "        token_clean = re.sub(r\"\\s+\", \"\", entry[\"token\"])\n",
    "        prev_len = cumulative_len\n",
    "        cumulative_len += len(token_clean)\n",
    "\n",
    "        if begin_index == -1 and cumulative_len > char_start_idx:\n",
    "            begin_index = idx\n",
    "        if cumulative_len >= char_start_idx + len(fragment_clean):\n",
    "            end_index = idx\n",
    "            return begin_index, end_index\n",
    "\n",
    "    return begin_index, len(token_probs) - 1  # fallback\n",
    "\n",
    "class JSONLProcessor:\n",
    "    \"\"\"\n",
    "    é«˜æ•ˆçš„JSONLå¤„ç†å™¨ï¼Œæ”¯æŒå†…å­˜ç®¡ç†å’Œè¿›åº¦è·Ÿè¸ª\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str, model: str = \"gpt-4.1\"):\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self.processed_count = 0\n",
    "        self.error_count = 0\n",
    "        self.start_time = None\n",
    "\n",
    "    def convert_json_to_jsonl(self, input_path: str, output_path: str,\n",
    "                             chunk_size: int = 1000):\n",
    "        \"\"\"\n",
    "        å°†å¤§JSONæ–‡ä»¶è½¬æ¢ä¸ºJSONLï¼Œæ”¯æŒåˆ†å—å¤„ç†\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ”„ Converting {input_path} to JSONL format...\")\n",
    "\n",
    "        # åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "        # æ£€æŸ¥æ–‡ä»¶å¤§å°\n",
    "        file_size = os.path.getsize(input_path)\n",
    "        print(f\"ğŸ“Š Input file size: {file_size / (1024**3):.2f} GB\")\n",
    "\n",
    "        with open(input_path, 'r', encoding='utf-8') as infile:\n",
    "            data = json.load(infile)\n",
    "\n",
    "        total_items = len(data)\n",
    "        print(f\"ğŸ“Š Total items to convert: {total_items}\")\n",
    "\n",
    "        with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "            for i, (qid, sample) in enumerate(data.items()):\n",
    "                line_data = {\n",
    "                    \"qid\": qid,\n",
    "                    \"data\": sample\n",
    "                }\n",
    "                outfile.write(json.dumps(line_data, ensure_ascii=False) + '\\n')\n",
    "\n",
    "                if (i + 1) % chunk_size == 0:\n",
    "                    print(f\"ğŸ“ˆ Converted {i + 1}/{total_items} items...\")\n",
    "                    # å¼ºåˆ¶åˆ·æ–°åˆ°ç£ç›˜\n",
    "                    outfile.flush()\n",
    "\n",
    "        print(f\"âœ… Conversion complete! Saved to {output_path}\")\n",
    "\n",
    "        # æ¸…ç†å†…å­˜\n",
    "        del data\n",
    "        gc.collect()\n",
    "\n",
    "    def process_jsonl_file(self, jsonl_path: str, output_path: str,\n",
    "                          batch_size: int = 10, save_interval: int = 20):\n",
    "        \"\"\"\n",
    "        æµå¼å¤„ç†JSONLæ–‡ä»¶ï¼Œæ”¯æŒæ‰¹å¤„ç†å’Œå®šæœŸä¿å­˜\n",
    "        \"\"\"\n",
    "        self.start_time = time.time()\n",
    "        results = {}\n",
    "\n",
    "        # å¦‚æœè¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼ŒåŠ è½½å·²å¤„ç†çš„ç»“æœ\n",
    "        if os.path.exists(output_path):\n",
    "            print(\"ğŸ“‚ Loading existing results...\")\n",
    "            try:\n",
    "                with open(output_path, 'r', encoding='utf-8') as f:\n",
    "                    results = json.load(f)\n",
    "                    self.processed_count = len(results)\n",
    "                    print(f\"ğŸ“Š Loaded {self.processed_count} existing results\")\n",
    "            except (json.JSONDecodeError, FileNotFoundError):\n",
    "                print(\"âš ï¸ Could not load existing results, starting fresh\")\n",
    "                results = {}\n",
    "\n",
    "        # åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            batch = []\n",
    "            line_count = 0\n",
    "\n",
    "            for line in f:\n",
    "                try:\n",
    "                    line_data = json.loads(line.strip())\n",
    "                    qid = line_data[\"qid\"]\n",
    "\n",
    "                    # è·³è¿‡å·²å¤„ç†çš„é¡¹ç›®\n",
    "                    if qid in results:\n",
    "                        print(f\"â­ï¸ Skipping already processed: {qid}\")\n",
    "                        continue\n",
    "\n",
    "                    batch.append((qid, line_data[\"data\"]))\n",
    "                    line_count += 1\n",
    "\n",
    "                    # å¤„ç†æ‰¹æ¬¡\n",
    "                    if len(batch) >= batch_size:\n",
    "                        self._process_batch(batch, results)\n",
    "                        batch = []\n",
    "\n",
    "                        # å®šæœŸä¿å­˜å’Œæ¸…ç†å†…å­˜\n",
    "                        if line_count % save_interval == 0:\n",
    "                            self._save_results(results, output_path)\n",
    "                            gc.collect()\n",
    "                            self._print_progress()\n",
    "\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"âš ï¸ JSON decode error in line: {e}\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Unexpected error processing line: {e}\")\n",
    "                    continue\n",
    "\n",
    "            # å¤„ç†å‰©ä½™çš„é¡¹ç›®\n",
    "            if batch:\n",
    "                self._process_batch(batch, results)\n",
    "\n",
    "        # æœ€ç»ˆä¿å­˜\n",
    "        self._save_results(results, output_path)\n",
    "        self._print_final_stats()\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _process_batch(self, batch: list, results: dict):\n",
    "        \"\"\"å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®\"\"\"\n",
    "        for qid, sample in batch:\n",
    "            try:\n",
    "                print(f\"ğŸ” Processing {qid}...\")\n",
    "                result = self._process_single_sample(qid, sample)\n",
    "                if result:\n",
    "                    results[qid] = result\n",
    "                    self.processed_count += 1\n",
    "                    print(f\"âœ… Successfully processed {qid}\")\n",
    "                else:\n",
    "                    print(f\"âš ï¸ No valid result for {qid}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error processing {qid}: {str(e)}\")\n",
    "                self.error_count += 1\n",
    "                continue\n",
    "\n",
    "    def _process_single_sample(self, qid: str, sample: dict) -> dict:\n",
    "        \"\"\"å¤„ç†å•ä¸ªæ ·æœ¬\"\"\"\n",
    "        try:\n",
    "            question = sample.get(\"question\", \"\")\n",
    "            true_final_result = sample.get(\"true_final_result\", \"\")\n",
    "\n",
    "            if not question or not true_final_result:\n",
    "                print(f\"âš ï¸ Missing question or true_final_result for {qid}\")\n",
    "                return None\n",
    "\n",
    "            # æ‰¾åˆ°æ­£æ ·æœ¬\n",
    "            correct_sampling_id = None\n",
    "            correct_sample_answer = None\n",
    "\n",
    "            for sampling_id in [\"sampling0\", \"sampling1\", \"sampling2\"]:\n",
    "                if sampling_id not in sample:\n",
    "                    continue\n",
    "                sampling_data = sample[sampling_id]\n",
    "                if sampling_data.get(\"final_result\") == true_final_result:\n",
    "                    correct_sampling_id = sampling_id\n",
    "                    correct_sample_answer = sampling_data.get(\"whole_answer\", \"\")\n",
    "                    break\n",
    "\n",
    "            if correct_sample_answer is None:\n",
    "                print(f\"âš ï¸ No correct sampling found for {qid}\")\n",
    "                return None\n",
    "\n",
    "            sample_results = {}\n",
    "\n",
    "            # å¤„ç†è´Ÿæ ·æœ¬\n",
    "            for sampling_id in [\"sampling0\", \"sampling1\", \"sampling2\"]:\n",
    "                if sampling_id not in sample:\n",
    "                    continue\n",
    "                sampling = sample[sampling_id]\n",
    "\n",
    "                # è·³è¿‡æ­£æ ·æœ¬\n",
    "                if sampling.get(\"final_result\") == true_final_result:\n",
    "                    continue\n",
    "\n",
    "                incorrect_sample_answer = sampling.get(\"whole_answer\", \"\")\n",
    "                if not incorrect_sample_answer:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    print(f\"  ğŸ” Analyzing {sampling_id}...\")\n",
    "\n",
    "                    # è°ƒç”¨API\n",
    "                    prompt = build_error_prompt(question, correct_sample_answer, incorrect_sample_answer)\n",
    "                    output = call_custom_gpt_api(prompt)\n",
    "\n",
    "                    # è§£æç»“æœ\n",
    "                    output = output.strip().strip(\"```\")\n",
    "                    if output.startswith(\"json\"):\n",
    "                        output = output[4:].strip()\n",
    "\n",
    "                    output_json = json.loads(output)\n",
    "\n",
    "                    # æŸ¥æ‰¾tokenç´¢å¼•\n",
    "                    error_sentence = output_json.get(\"first_error_sentence\", \"\")\n",
    "                    fix_sentence = output_json.get(\"fix_sentence\", \"\")\n",
    "\n",
    "                    error_token_probs = sampling.get(\"token_probs\", [])\n",
    "                    correct_token_probs = sample[correct_sampling_id].get(\"token_probs\", [])\n",
    "\n",
    "                    error_begin_idx, error_end_idx = find_sentence_span_indices_robust(\n",
    "                        error_sentence, error_token_probs\n",
    "                    )\n",
    "                    fix_begin_idx, fix_end_idx = find_sentence_span_indices_robust(\n",
    "                        fix_sentence, correct_token_probs\n",
    "                    )\n",
    "\n",
    "                    sample_results[sampling_id] = {\n",
    "                        \"first_error_sentence\": error_sentence,\n",
    "                        \"error_reason\": output_json.get(\"error_reason\", \"\"),\n",
    "                        \"fix_sentence\": fix_sentence,\n",
    "                        \"fix_reason\": output_json.get(\"fix_reason\", \"\"),\n",
    "                        \"correct_sampling_id\": correct_sampling_id,\n",
    "                        \"error_token_begin_index\": error_begin_idx,\n",
    "                        \"error_token_end_index\": error_end_idx,\n",
    "                        \"fix_token_begin_index\": fix_begin_idx,\n",
    "                        \"fix_token_end_index\": fix_end_idx\n",
    "                    }\n",
    "\n",
    "                    print(f\"  âœ… Successfully analyzed {sampling_id}\")\n",
    "\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"  âš ï¸ JSON decode error for {sampling_id}: {e}\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"  âš ï¸ Error analyzing {sampling_id}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            return sample_results if sample_results else None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error in _process_single_sample for {qid}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _save_results(self, results: dict, output_path: str):\n",
    "        \"\"\"ä¿å­˜ç»“æœåˆ°æ–‡ä»¶\"\"\"\n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"ğŸ’¾ Saved {len(results)} results to {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error saving results: {e}\")\n",
    "\n",
    "    def _print_progress(self):\n",
    "        \"\"\"æ‰“å°è¿›åº¦ä¿¡æ¯\"\"\"\n",
    "        elapsed = time.time() - self.start_time\n",
    "        speed = self.processed_count / elapsed if elapsed > 0 else 0\n",
    "        print(f\"ğŸ“Š Progress: {self.processed_count} processed, {self.error_count} errors, \"\n",
    "              f\"{speed:.2f} items/sec, {elapsed:.1f}s elapsed\")\n",
    "\n",
    "    def _print_final_stats(self):\n",
    "        \"\"\"æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
    "        elapsed = time.time() - self.start_time\n",
    "        print(f\"\\nğŸ‰ Processing complete!\")\n",
    "        print(f\"ğŸ“Š Total processed: {self.processed_count}\")\n",
    "        print(f\"âš ï¸ Total errors: {self.error_count}\")\n",
    "        print(f\"â±ï¸ Total time: {elapsed:.1f}s\")\n",
    "        if elapsed > 0:\n",
    "            print(f\"ğŸš€ Average speed: {self.processed_count / elapsed:.2f} items/sec\")\n",
    "\n",
    "# âœ… ä¸»å‡½æ•°\n",
    "def main():\n",
    "    # è·¯å¾„é…ç½®\n",
    "    \n",
    "    range_tag = \"0-15\"\n",
    "    level = \"level_3\"\n",
    "    discipline = \"algebra\"\n",
    "    input_json = f\"../logits/deepseek-math-7b-math-{range_tag}-{discipline}-{level}.json\"\n",
    "    output_jsonl = f\"../logits/deepseek-math-7b-math-{range_tag}-{discipline}-{level}.jsonl\"\n",
    "    output_results = f\"../error_fix_index/deepseek-math-7b-math-{range_tag}-{discipline}-{level}_index.json\"\n",
    "\n",
    "    # åˆ›å»ºå¤„ç†å™¨\n",
    "    processor = JSONLProcessor(API_KEY, MODEL)\n",
    "\n",
    "    # æ­¥éª¤1: è½¬æ¢ä¸ºJSONLï¼ˆå¦‚æœè¿˜æ²¡æœ‰è½¬æ¢ï¼‰\n",
    "    if not os.path.exists(output_jsonl):\n",
    "        processor.convert_json_to_jsonl(input_json, output_jsonl)\n",
    "    else:\n",
    "        print(f\"ğŸ“‚ JSONL file already exists: {output_jsonl}\")\n",
    "\n",
    "    # æ­¥éª¤2: å¤„ç†JSONLæ–‡ä»¶\n",
    "    print(\"\\nğŸš€ Starting JSONL processing...\")\n",
    "    results = processor.process_jsonl_file(\n",
    "        jsonl_path=output_jsonl,\n",
    "        output_path=output_results,\n",
    "        batch_size=1,      # è®¾ç½®ä¸º1ä»¥ä¾¿è°ƒè¯•\n",
    "        save_interval=5    # æ¯5ä¸ªé¡¹ç›®ä¿å­˜ä¸€æ¬¡\n",
    "    )\n",
    "\n",
    "    print(f\"\\nâœ… All processing complete! Results saved to {output_results}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMQPNedmW8IUavg3TIqI+SP",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cluster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

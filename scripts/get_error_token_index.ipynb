{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMQPNedmW8IUavg3TIqI+SP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rnyWRoWaqg-O","executionInfo":{"status":"ok","timestamp":1752393886778,"user_tz":-480,"elapsed":21243,"user":{"displayName":"Tingting Du","userId":"01262363838823204487"}},"outputId":"2b2200f5-7d63-42a9-bc84-5c2d5d362c04"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#version 1\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Complete JSONL processor with all required functions\n","\"\"\"\n","\n","import json\n","import os\n","import gc\n","import requests\n","import re\n","import time\n","from pathlib import Path\n","from typing import Iterator, Dict, Any\n","\n","# âœ… APIé…ç½®\n","API_KEY = \"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\"\n","MODEL = \"gpt-4.1\"\n","\n","# âœ… æ„å»ºé”™è¯¯åˆ†ææç¤ºè¯\n","def build_error_prompt(question, true_whole_answer, sample_whole_answer):\n","    \"\"\"æ„å»ºç”¨äºé”™è¯¯åˆ†æçš„æç¤ºè¯\"\"\"\n","    return f\"\"\"\n","Here is a math problem, its correct answer, and a sample answer that may contain mistakes.\n","\n","ã€Questionã€‘:\n","{question}\n","\n","ã€Correct Answerã€‘:\n","{true_whole_answer}\n","\n","ã€Incorrect Answerã€‘:\n","{sample_whole_answer}\n","\n","Please help me:\n","1. Identify the earliest mistake in the incorrect answer and provide the complete sentence from that point.\n","2. Briefly explain why it is incorrect.\n","3. Find the fix sentence in correct answer that and fix the error.\n","4. Briefly explain why it can fix the error.\n","\n","Please output in the following JSON format:\n","{{\n","  \"first_error_sentence\": \"<sentence>\",\n","  \"error_reason\": \"<brief explanation>\",\n","  \"fix_sentence\": \"<sentence>\",\n","  \"fix_reason\": \"<brief explanation>\"\n","}}\n","\"\"\"\n","\n","# âœ… è°ƒç”¨GPT API\n","def call_custom_gpt_api(prompt):\n","    \"\"\"è°ƒç”¨OpenAI API\"\"\"\n","    headers = {\n","        \"Authorization\": f\"Bearer {API_KEY}\",\n","        \"Content-Type\": \"application/json\"\n","    }\n","    payload = {\n","        \"model\": MODEL,\n","        \"messages\": [\n","            {\"role\": \"system\", \"content\": \"You are a meticulous and precise comparer.\"},\n","            {\"role\": \"user\", \"content\": prompt}\n","        ]\n","    }\n","\n","    try:\n","        response = requests.post(\n","            \"https://api.openai.com/v1/chat/completions\",\n","            headers=headers,\n","            json=payload,\n","            timeout=30  # æ·»åŠ è¶…æ—¶\n","        )\n","\n","        if response.status_code != 200:\n","            raise Exception(f\"API request failed: {response.status_code}, {response.text}\")\n","\n","        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n","\n","    except requests.exceptions.Timeout:\n","        raise Exception(\"API request timeout\")\n","    except requests.exceptions.RequestException as e:\n","        raise Exception(f\"API request error: {str(e)}\")\n","\n","# âœ… æŸ¥æ‰¾å¥å­åœ¨tokenåºåˆ—ä¸­çš„ä½ç½®\n","def find_sentence_span_indices_robust(fragment, token_probs):\n","    \"\"\"\n","    è¿”å› fragment åœ¨ token_probs ä¸­åŒ¹é…åˆ°çš„ token èŒƒå›´: (begin_index, end_index)\n","    ä½¿ç”¨å»é™¤ç©ºç™½å­—ç¬¦çš„æ–¹å¼åŒ¹é…\n","    \"\"\"\n","    if not fragment or not token_probs:\n","        return -1, -1\n","\n","    fragment_clean = re.sub(r\"\\s+\", \"\", fragment)\n","    tokens = [entry[\"token\"] for entry in token_probs]\n","    decoded_text = \"\".join(tokens)\n","    decoded_text_clean = re.sub(r\"\\s+\", \"\", decoded_text)\n","\n","    char_start_idx = decoded_text_clean.find(fragment_clean)\n","    if char_start_idx == -1:\n","        return -1, -1\n","\n","    cumulative_len = 0\n","    begin_index = -1\n","\n","    for idx, entry in enumerate(token_probs):\n","        token_clean = re.sub(r\"\\s+\", \"\", entry[\"token\"])\n","        prev_len = cumulative_len\n","        cumulative_len += len(token_clean)\n","\n","        if begin_index == -1 and cumulative_len > char_start_idx:\n","            begin_index = idx\n","        if cumulative_len >= char_start_idx + len(fragment_clean):\n","            end_index = idx\n","            return begin_index, end_index\n","\n","    return begin_index, len(token_probs) - 1  # fallback\n","\n","class JSONLProcessor:\n","    \"\"\"\n","    é«˜æ•ˆçš„JSONLå¤„ç†å™¨ï¼Œæ”¯æŒå†…å­˜ç®¡ç†å’Œè¿›åº¦è·Ÿè¸ª\n","    \"\"\"\n","\n","    def __init__(self, api_key: str, model: str = \"gpt-4.1\"):\n","        self.api_key = api_key\n","        self.model = model\n","        self.processed_count = 0\n","        self.error_count = 0\n","        self.start_time = None\n","\n","    def convert_json_to_jsonl(self, input_path: str, output_path: str,\n","                             chunk_size: int = 1000):\n","        \"\"\"\n","        å°†å¤§JSONæ–‡ä»¶è½¬æ¢ä¸ºJSONLï¼Œæ”¯æŒåˆ†å—å¤„ç†\n","        \"\"\"\n","        print(f\"ğŸ”„ Converting {input_path} to JSONL format...\")\n","\n","        # åˆ›å»ºè¾“å‡ºç›®å½•\n","        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","\n","        # æ£€æŸ¥æ–‡ä»¶å¤§å°\n","        file_size = os.path.getsize(input_path)\n","        print(f\"ğŸ“Š Input file size: {file_size / (1024**3):.2f} GB\")\n","\n","        with open(input_path, 'r', encoding='utf-8') as infile:\n","            data = json.load(infile)\n","\n","        total_items = len(data)\n","        print(f\"ğŸ“Š Total items to convert: {total_items}\")\n","\n","        with open(output_path, 'w', encoding='utf-8') as outfile:\n","            for i, (qid, sample) in enumerate(data.items()):\n","                line_data = {\n","                    \"qid\": qid,\n","                    \"data\": sample\n","                }\n","                outfile.write(json.dumps(line_data, ensure_ascii=False) + '\\n')\n","\n","                if (i + 1) % chunk_size == 0:\n","                    print(f\"ğŸ“ˆ Converted {i + 1}/{total_items} items...\")\n","                    # å¼ºåˆ¶åˆ·æ–°åˆ°ç£ç›˜\n","                    outfile.flush()\n","\n","        print(f\"âœ… Conversion complete! Saved to {output_path}\")\n","\n","        # æ¸…ç†å†…å­˜\n","        del data\n","        gc.collect()\n","\n","    def process_jsonl_file(self, jsonl_path: str, output_path: str,\n","                          batch_size: int = 10, save_interval: int = 20):\n","        \"\"\"\n","        æµå¼å¤„ç†JSONLæ–‡ä»¶ï¼Œæ”¯æŒæ‰¹å¤„ç†å’Œå®šæœŸä¿å­˜\n","        \"\"\"\n","        self.start_time = time.time()\n","        results = {}\n","\n","        # å¦‚æœè¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼ŒåŠ è½½å·²å¤„ç†çš„ç»“æœ\n","        if os.path.exists(output_path):\n","            print(\"ğŸ“‚ Loading existing results...\")\n","            try:\n","                with open(output_path, 'r', encoding='utf-8') as f:\n","                    results = json.load(f)\n","                    self.processed_count = len(results)\n","                    print(f\"ğŸ“Š Loaded {self.processed_count} existing results\")\n","            except (json.JSONDecodeError, FileNotFoundError):\n","                print(\"âš ï¸ Could not load existing results, starting fresh\")\n","                results = {}\n","\n","        # åˆ›å»ºè¾“å‡ºç›®å½•\n","        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","\n","        with open(jsonl_path, 'r', encoding='utf-8') as f:\n","            batch = []\n","            line_count = 0\n","\n","            for line in f:\n","                try:\n","                    line_data = json.loads(line.strip())\n","                    qid = line_data[\"qid\"]\n","\n","                    # è·³è¿‡å·²å¤„ç†çš„é¡¹ç›®\n","                    if qid in results:\n","                        print(f\"â­ï¸ Skipping already processed: {qid}\")\n","                        continue\n","\n","                    batch.append((qid, line_data[\"data\"]))\n","                    line_count += 1\n","\n","                    # å¤„ç†æ‰¹æ¬¡\n","                    if len(batch) >= batch_size:\n","                        self._process_batch(batch, results)\n","                        batch = []\n","\n","                        # å®šæœŸä¿å­˜å’Œæ¸…ç†å†…å­˜\n","                        if line_count % save_interval == 0:\n","                            self._save_results(results, output_path)\n","                            gc.collect()\n","                            self._print_progress()\n","\n","                except json.JSONDecodeError as e:\n","                    print(f\"âš ï¸ JSON decode error in line: {e}\")\n","                    continue\n","                except Exception as e:\n","                    print(f\"âš ï¸ Unexpected error processing line: {e}\")\n","                    continue\n","\n","            # å¤„ç†å‰©ä½™çš„é¡¹ç›®\n","            if batch:\n","                self._process_batch(batch, results)\n","\n","        # æœ€ç»ˆä¿å­˜\n","        self._save_results(results, output_path)\n","        self._print_final_stats()\n","\n","        return results\n","\n","    def _process_batch(self, batch: list, results: dict):\n","        \"\"\"å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®\"\"\"\n","        for qid, sample in batch:\n","            try:\n","                print(f\"ğŸ” Processing {qid}...\")\n","                result = self._process_single_sample(qid, sample)\n","                if result:\n","                    results[qid] = result\n","                    self.processed_count += 1\n","                    print(f\"âœ… Successfully processed {qid}\")\n","                else:\n","                    print(f\"âš ï¸ No valid result for {qid}\")\n","\n","            except Exception as e:\n","                print(f\"âš ï¸ Error processing {qid}: {str(e)}\")\n","                self.error_count += 1\n","                continue\n","\n","    def _process_single_sample(self, qid: str, sample: dict) -> dict:\n","        \"\"\"å¤„ç†å•ä¸ªæ ·æœ¬\"\"\"\n","        try:\n","            question = sample.get(\"question\", \"\")\n","            true_final_result = sample.get(\"true_final_result\", \"\")\n","\n","            if not question or not true_final_result:\n","                print(f\"âš ï¸ Missing question or true_final_result for {qid}\")\n","                return None\n","\n","            # æ‰¾åˆ°æ­£æ ·æœ¬\n","            correct_sampling_id = None\n","            correct_sample_answer = None\n","\n","            for sampling_id in [\"sampling0\", \"sampling1\", \"sampling2\"]:\n","                if sampling_id not in sample:\n","                    continue\n","                sampling_data = sample[sampling_id]\n","                if sampling_data.get(\"final_result\") == true_final_result:\n","                    correct_sampling_id = sampling_id\n","                    correct_sample_answer = sampling_data.get(\"whole_answer\", \"\")\n","                    break\n","\n","            if correct_sample_answer is None:\n","                print(f\"âš ï¸ No correct sampling found for {qid}\")\n","                return None\n","\n","            sample_results = {}\n","\n","            # å¤„ç†è´Ÿæ ·æœ¬\n","            for sampling_id in [\"sampling0\", \"sampling1\", \"sampling2\"]:\n","                if sampling_id not in sample:\n","                    continue\n","                sampling = sample[sampling_id]\n","\n","                # è·³è¿‡æ­£æ ·æœ¬\n","                if sampling.get(\"final_result\") == true_final_result:\n","                    continue\n","\n","                incorrect_sample_answer = sampling.get(\"whole_answer\", \"\")\n","                if not incorrect_sample_answer:\n","                    continue\n","\n","                try:\n","                    print(f\"  ğŸ” Analyzing {sampling_id}...\")\n","\n","                    # è°ƒç”¨API\n","                    prompt = build_error_prompt(question, correct_sample_answer, incorrect_sample_answer)\n","                    output = call_custom_gpt_api(prompt)\n","\n","                    # è§£æç»“æœ\n","                    output = output.strip().strip(\"```\")\n","                    if output.startswith(\"json\"):\n","                        output = output[4:].strip()\n","\n","                    output_json = json.loads(output)\n","\n","                    # æŸ¥æ‰¾tokenç´¢å¼•\n","                    error_sentence = output_json.get(\"first_error_sentence\", \"\")\n","                    fix_sentence = output_json.get(\"fix_sentence\", \"\")\n","\n","                    error_token_probs = sampling.get(\"token_probs\", [])\n","                    correct_token_probs = sample[correct_sampling_id].get(\"token_probs\", [])\n","\n","                    error_begin_idx, error_end_idx = find_sentence_span_indices_robust(\n","                        error_sentence, error_token_probs\n","                    )\n","                    fix_begin_idx, fix_end_idx = find_sentence_span_indices_robust(\n","                        fix_sentence, correct_token_probs\n","                    )\n","\n","                    sample_results[sampling_id] = {\n","                        \"first_error_sentence\": error_sentence,\n","                        \"error_reason\": output_json.get(\"error_reason\", \"\"),\n","                        \"fix_sentence\": fix_sentence,\n","                        \"fix_reason\": output_json.get(\"fix_reason\", \"\"),\n","                        \"correct_sampling_id\": correct_sampling_id,\n","                        \"error_token_begin_index\": error_begin_idx,\n","                        \"error_token_end_index\": error_end_idx,\n","                        \"fix_token_begin_index\": fix_begin_idx,\n","                        \"fix_token_end_index\": fix_end_idx\n","                    }\n","\n","                    print(f\"  âœ… Successfully analyzed {sampling_id}\")\n","\n","                except json.JSONDecodeError as e:\n","                    print(f\"  âš ï¸ JSON decode error for {sampling_id}: {e}\")\n","                    continue\n","                except Exception as e:\n","                    print(f\"  âš ï¸ Error analyzing {sampling_id}: {e}\")\n","                    continue\n","\n","            return sample_results if sample_results else None\n","\n","        except Exception as e:\n","            print(f\"âš ï¸ Error in _process_single_sample for {qid}: {e}\")\n","            return None\n","\n","    def _save_results(self, results: dict, output_path: str):\n","        \"\"\"ä¿å­˜ç»“æœåˆ°æ–‡ä»¶\"\"\"\n","        try:\n","            with open(output_path, 'w', encoding='utf-8') as f:\n","                json.dump(results, f, ensure_ascii=False, indent=2)\n","            print(f\"ğŸ’¾ Saved {len(results)} results to {output_path}\")\n","        except Exception as e:\n","            print(f\"âš ï¸ Error saving results: {e}\")\n","\n","    def _print_progress(self):\n","        \"\"\"æ‰“å°è¿›åº¦ä¿¡æ¯\"\"\"\n","        elapsed = time.time() - self.start_time\n","        speed = self.processed_count / elapsed if elapsed > 0 else 0\n","        print(f\"ğŸ“Š Progress: {self.processed_count} processed, {self.error_count} errors, \"\n","              f\"{speed:.2f} items/sec, {elapsed:.1f}s elapsed\")\n","\n","    def _print_final_stats(self):\n","        \"\"\"æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯\"\"\"\n","        elapsed = time.time() - self.start_time\n","        print(f\"\\nğŸ‰ Processing complete!\")\n","        print(f\"ğŸ“Š Total processed: {self.processed_count}\")\n","        print(f\"âš ï¸ Total errors: {self.error_count}\")\n","        print(f\"â±ï¸ Total time: {elapsed:.1f}s\")\n","        if elapsed > 0:\n","            print(f\"ğŸš€ Average speed: {self.processed_count / elapsed:.2f} items/sec\")\n","\n","# âœ… ä¸»å‡½æ•°\n","def main():\n","    # è·¯å¾„é…ç½®\n","    BASE_PATH = \"/content/drive/MyDrive/Cluster-proj\"\n","    range_tag = \"700-731\"\n","\n","    input_json = f\"{BASE_PATH}/output/llm_steps/whole_logits/deepseek7b-gsm-{range_tag}-hidden.json\"\n","    output_jsonl = f\"{BASE_PATH}/output/llm_steps/whole_logits/deepseek7b-gsm-{range_tag}.jsonl\"\n","    output_results = f\"{BASE_PATH}/output/error_fix_index/deepseek-7b-{range_tag}_error_fix_index.json\"\n","\n","    # åˆ›å»ºå¤„ç†å™¨\n","    processor = JSONLProcessor(API_KEY, MODEL)\n","\n","    # æ­¥éª¤1: è½¬æ¢ä¸ºJSONLï¼ˆå¦‚æœè¿˜æ²¡æœ‰è½¬æ¢ï¼‰\n","    if not os.path.exists(output_jsonl):\n","        processor.convert_json_to_jsonl(input_json, output_jsonl)\n","    else:\n","        print(f\"ğŸ“‚ JSONL file already exists: {output_jsonl}\")\n","\n","    # æ­¥éª¤2: å¤„ç†JSONLæ–‡ä»¶\n","    print(\"\\nğŸš€ Starting JSONL processing...\")\n","    results = processor.process_jsonl_file(\n","        jsonl_path=output_jsonl,\n","        output_path=output_results,\n","        batch_size=1,      # è®¾ç½®ä¸º1ä»¥ä¾¿è°ƒè¯•\n","        save_interval=5    # æ¯5ä¸ªé¡¹ç›®ä¿å­˜ä¸€æ¬¡\n","    )\n","\n","    print(f\"\\nâœ… All processing complete! Results saved to {output_results}\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"olcts10oX0nL","executionInfo":{"status":"ok","timestamp":1752394079714,"user_tz":-480,"elapsed":192933,"user":{"displayName":"Tingting Du","userId":"01262363838823204487"}},"outputId":"62b860f6-cd75-4ba6-bc60-56edc268efb1"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ”„ Converting /content/drive/MyDrive/Cluster-proj/output/llm_steps/whole_logits/deepseek7b-gsm-700-731-hidden.json to JSONL format...\n","ğŸ“Š Input file size: 1.79 GB\n","ğŸ“Š Total items to convert: 31\n","âœ… Conversion complete! Saved to /content/drive/MyDrive/Cluster-proj/output/llm_steps/whole_logits/deepseek7b-gsm-700-731.jsonl\n","\n","ğŸš€ Starting JSONL processing...\n","ğŸ” Processing q_700...\n","  ğŸ” Analyzing sampling0...\n","  âœ… Successfully analyzed sampling0\n","  ğŸ” Analyzing sampling2...\n","  âœ… Successfully analyzed sampling2\n","âœ… Successfully processed q_700\n","ğŸ” Processing q_701...\n","  ğŸ” Analyzing sampling1...\n","  âœ… Successfully analyzed sampling1\n","âœ… Successfully processed q_701\n","ğŸ” Processing q_702...\n","âš ï¸ No valid result for q_702\n","ğŸ” Processing q_703...\n","  ğŸ” Analyzing sampling1...\n","  âœ… Successfully analyzed sampling1\n","âœ… Successfully processed q_703\n","ğŸ” Processing q_704...\n","âš ï¸ No valid result for q_704\n","ğŸ’¾ Saved 3 results to /content/drive/MyDrive/Cluster-proj/output/error_fix_index/deepseek-7b-700-731_error_fix_index.json\n","ğŸ“Š Progress: 3 processed, 0 errors, 0.21 items/sec, 14.4s elapsed\n","ğŸ” Processing q_705...\n","  ğŸ” Analyzing sampling1...\n","  âœ… Successfully analyzed sampling1\n","  ğŸ” Analyzing sampling2...\n","  âœ… Successfully analyzed sampling2\n","âœ… Successfully processed q_705\n","ğŸ” Processing q_706...\n","âš ï¸ No valid result for q_706\n","ğŸ” Processing q_707...\n","  ğŸ” Analyzing sampling0...\n","  âœ… Successfully analyzed sampling0\n","  ğŸ” Analyzing sampling2...\n","  âœ… Successfully analyzed sampling2\n","âœ… Successfully processed q_707\n","ğŸ” Processing q_708...\n","âš ï¸ No correct sampling found for q_708\n","âš ï¸ No valid result for q_708\n","ğŸ” Processing q_709...\n","  ğŸ” Analyzing sampling0...\n","  âœ… Successfully analyzed sampling0\n","  ğŸ” Analyzing sampling2...\n","  âœ… Successfully analyzed sampling2\n","âœ… Successfully processed q_709\n","ğŸ’¾ Saved 6 results to /content/drive/MyDrive/Cluster-proj/output/error_fix_index/deepseek-7b-700-731_error_fix_index.json\n","ğŸ“Š Progress: 6 processed, 0 errors, 0.19 items/sec, 31.7s elapsed\n","ğŸ” Processing q_710...\n","  ğŸ” Analyzing sampling1...\n","  âœ… Successfully analyzed sampling1\n","âœ… Successfully processed q_710\n","ğŸ” Processing q_711...\n","  ğŸ” Analyzing sampling0...\n","  âœ… Successfully analyzed sampling0\n","âœ… Successfully processed q_711\n","ğŸ” Processing q_712...\n","  ğŸ” Analyzing sampling0...\n","  âœ… Successfully analyzed sampling0\n","  ğŸ” Analyzing sampling1...\n","  âœ… Successfully analyzed sampling1\n","âœ… Successfully processed q_712\n","ğŸ” Processing q_713...\n","âš ï¸ No valid result for q_713\n","ğŸ” Processing q_714...\n","  ğŸ” Analyzing sampling0...\n","  âœ… Successfully analyzed sampling0\n","  ğŸ” Analyzing sampling1...\n","  âœ… Successfully analyzed sampling1\n","âœ… Successfully processed q_714\n","ğŸ’¾ Saved 10 results to /content/drive/MyDrive/Cluster-proj/output/error_fix_index/deepseek-7b-700-731_error_fix_index.json\n","ğŸ“Š Progress: 10 processed, 0 errors, 0.22 items/sec, 44.8s elapsed\n","ğŸ” Processing q_715...\n","  ğŸ” Analyzing sampling1...\n","  âœ… Successfully analyzed sampling1\n","âœ… Successfully processed q_715\n","ğŸ” Processing q_716...\n","âš ï¸ No correct sampling found for q_716\n","âš ï¸ No valid result for q_716\n","ğŸ” Processing q_717...\n","  ğŸ” Analyzing sampling1...\n","  âœ… Successfully analyzed sampling1\n","âœ… Successfully processed q_717\n","ğŸ” Processing q_718...\n","  ğŸ” Analyzing sampling2...\n","  âœ… Successfully analyzed sampling2\n","âœ… Successfully processed q_718\n","ğŸ” Processing q_719...\n","âš ï¸ No valid result for q_719\n","ğŸ’¾ Saved 13 results to /content/drive/MyDrive/Cluster-proj/output/error_fix_index/deepseek-7b-700-731_error_fix_index.json\n","ğŸ“Š Progress: 13 processed, 0 errors, 0.23 items/sec, 57.0s elapsed\n","ğŸ” Processing q_720...\n","  ğŸ” Analyzing sampling1...\n","  âœ… Successfully analyzed sampling1\n","âœ… Successfully processed q_720\n","ğŸ” Processing q_721...\n","  ğŸ” Analyzing sampling2...\n","  âœ… Successfully analyzed sampling2\n","âœ… Successfully processed q_721\n","ğŸ” Processing q_722...\n","âš ï¸ No valid result for q_722\n","ğŸ” Processing q_723...\n","  ğŸ” Analyzing sampling2...\n","  âœ… Successfully analyzed sampling2\n","âœ… Successfully processed q_723\n","ğŸ” Processing q_724...\n","  ğŸ” Analyzing sampling0...\n","  âœ… Successfully analyzed sampling0\n","  ğŸ” Analyzing sampling2...\n","  âœ… Successfully analyzed sampling2\n","âœ… Successfully processed q_724\n","ğŸ’¾ Saved 17 results to /content/drive/MyDrive/Cluster-proj/output/error_fix_index/deepseek-7b-700-731_error_fix_index.json\n","ğŸ“Š Progress: 17 processed, 0 errors, 0.24 items/sec, 70.8s elapsed\n","ğŸ” Processing q_725...\n","âš ï¸ No valid result for q_725\n","ğŸ” Processing q_726...\n","âš ï¸ No correct sampling found for q_726\n","âš ï¸ No valid result for q_726\n","ğŸ” Processing q_727...\n","  ğŸ” Analyzing sampling0...\n","  âœ… Successfully analyzed sampling0\n","  ğŸ” Analyzing sampling1...\n","  âœ… Successfully analyzed sampling1\n","âœ… Successfully processed q_727\n","ğŸ” Processing q_728...\n","âš ï¸ No valid result for q_728\n","ğŸ” Processing q_729...\n","âš ï¸ No valid result for q_729\n","ğŸ’¾ Saved 18 results to /content/drive/MyDrive/Cluster-proj/output/error_fix_index/deepseek-7b-700-731_error_fix_index.json\n","ğŸ“Š Progress: 18 processed, 0 errors, 0.23 items/sec, 76.8s elapsed\n","ğŸ” Processing q_730...\n","  ğŸ” Analyzing sampling2...\n","  âœ… Successfully analyzed sampling2\n","âœ… Successfully processed q_730\n","ğŸ’¾ Saved 19 results to /content/drive/MyDrive/Cluster-proj/output/error_fix_index/deepseek-7b-700-731_error_fix_index.json\n","\n","ğŸ‰ Processing complete!\n","ğŸ“Š Total processed: 19\n","âš ï¸ Total errors: 0\n","â±ï¸ Total time: 78.9s\n","ğŸš€ Average speed: 0.24 items/sec\n","\n","âœ… All processing complete! Results saved to /content/drive/MyDrive/Cluster-proj/output/error_fix_index/deepseek-7b-700-731_error_fix_index.json\n"]}]},{"cell_type":"code","source":["\n","BASE_PATH = \"/content/drive/MyDrive/Cluster-proj\"\n","LOGITS_PATH = f\"{BASE_PATH}/output/llm_steps/whole_logits/deepseek-math-7b-gsm-{range_tag}.json\"\n","\n","# âœ… Prompt builder\n","def build_error_prompt(question, true_whole_answer, sample_whole_answer):\n","    return f\"\"\"\n","Here is a math problem, its correct answer, and a sample answer that may contain mistakes.\n","\n","ã€Questionã€‘:\n","{question}\n","\n","ã€Correct Answerã€‘:\n","{true_whole_answer}\n","\n","ã€Incorrect Answerã€‘:\n","{sample_whole_answer}\n","\n","Please help me:\n","1. Identify the earliest mistake in the incorrect answer and provide the compelete sentence from that point.\n","2. Briefly explain why it is incorrect.\n","3. Find the fix sentence in correct answer that and fix the error.\n","4. Briefly explain why it can fix the error.\n","\n","Please output in the following JSON format:\n","{{\n","  \"first_error_sentence\": \"<sentence>\",\n","  \"error_reason\": \"<brief explanation>\",\n","  \"fix_sentence\": \"<sentence>\",\n","  \"fix_reason\": \"<brief explanation>\"\n","}}\n","\"\"\"\n","\n","\n","\n","# âœ… Call your custom GPT API\n","def call_custom_gpt_api(prompt):\n","    headers = {\n","        \"Authorization\": f\"Bearer {API_KEY}\",\n","        \"Content-Type\": \"application/json\"\n","    }\n","    payload = {\n","        \"model\": MODEL,\n","        \"messages\": [\n","            {\"role\": \"system\", \"content\": \"You are a meticulous and precise comparer.\"},\n","            {\"role\": \"user\", \"content\": prompt}\n","        ]\n","    }\n","    response = requests.post(\n","        \"https://api.openai.com/v1/chat/completions\",\n","        headers=headers,\n","        json=payload\n","    )\n","    if response.status_code != 200:\n","        raise Exception(f\"API request failed: {response.status_code}, {response.text}\")\n","    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n","\n"],"metadata":{"id":"lYDdlp73XrYe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#version 2\n","\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","Advanced JSONL processing with memory management and progress tracking\n","\"\"\"\n","\n","import json\n","import os\n","import gc\n","from pathlib import Path\n","import time\n","from typing import Iterator, Dict, Any\n","\n","class JSONLProcessor:\n","    \"\"\"\n","    é«˜æ•ˆçš„JSONLå¤„ç†å™¨ï¼Œæ”¯æŒå†…å­˜ç®¡ç†å’Œè¿›åº¦è·Ÿè¸ª\n","    \"\"\"\n","\n","    def __init__(self, api_key: str, model: str = \"gpt-4.1\"):\n","        self.api_key = api_key\n","        self.model = model\n","        self.processed_count = 0\n","        self.error_count = 0\n","        self.start_time = None\n","\n","    def convert_json_to_jsonl(self, input_path: str, output_path: str,\n","                             chunk_size: int = 1000):\n","        \"\"\"\n","        å°†å¤§JSONæ–‡ä»¶è½¬æ¢ä¸ºJSONLï¼Œæ”¯æŒåˆ†å—å¤„ç†\n","        \"\"\"\n","        print(f\"ğŸ”„ Converting {input_path} to JSONL format...\")\n","\n","        # åˆ›å»ºè¾“å‡ºç›®å½•\n","        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","\n","        # æ£€æŸ¥æ–‡ä»¶å¤§å°\n","        file_size = os.path.getsize(input_path)\n","        print(f\"ğŸ“Š Input file size: {file_size / (1024**3):.2f} GB\")\n","\n","        with open(input_path, 'r', encoding='utf-8') as infile:\n","            data = json.load(infile)\n","\n","        total_items = len(data)\n","        print(f\"ğŸ“Š Total items to convert: {total_items}\")\n","\n","        with open(output_path, 'w', encoding='utf-8') as outfile:\n","            for i, (qid, sample) in enumerate(data.items()):\n","                line_data = {\n","                    \"qid\": qid,\n","                    \"data\": sample\n","                }\n","                outfile.write(json.dumps(line_data, ensure_ascii=False) + '\\n')\n","\n","                if (i + 1) % chunk_size == 0:\n","                    print(f\"ğŸ“ˆ Converted {i + 1}/{total_items} items...\")\n","                    # å¼ºåˆ¶åˆ·æ–°åˆ°ç£ç›˜\n","                    outfile.flush()\n","\n","        print(f\"âœ… Conversion complete! Saved to {output_path}\")\n","\n","        # æ¸…ç†å†…å­˜\n","        del data\n","        gc.collect()\n","\n","    def process_jsonl_file(self, jsonl_path: str, output_path: str,\n","                          batch_size: int = 10, save_interval: int = 50):\n","        \"\"\"\n","        æµå¼å¤„ç†JSONLæ–‡ä»¶ï¼Œæ”¯æŒæ‰¹å¤„ç†å’Œå®šæœŸä¿å­˜\n","        \"\"\"\n","        self.start_time = time.time()\n","        results = {}\n","\n","        # å¦‚æœè¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼ŒåŠ è½½å·²å¤„ç†çš„ç»“æœ\n","        if os.path.exists(output_path):\n","            print(\"ğŸ“‚ Loading existing results...\")\n","            with open(output_path, 'r', encoding='utf-8') as f:\n","                results = json.load(f)\n","                self.processed_count = len(results)\n","                print(f\"ğŸ“Š Loaded {self.processed_count} existing results\")\n","\n","        # åˆ›å»ºè¾“å‡ºç›®å½•\n","        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","\n","        with open(jsonl_path, 'r', encoding='utf-8') as f:\n","            batch = []\n","\n","            for line in f:\n","                line_data = json.loads(line.strip())\n","                qid = line_data[\"qid\"]\n","\n","                # è·³è¿‡å·²å¤„ç†çš„é¡¹ç›®\n","                if qid in results:\n","                    continue\n","\n","                batch.append((qid, line_data[\"data\"]))\n","\n","                # å¤„ç†æ‰¹æ¬¡\n","                if len(batch) >= batch_size:\n","                    self._process_batch(batch, results)\n","                    batch = []\n","\n","                    # å®šæœŸä¿å­˜å’Œæ¸…ç†å†…å­˜\n","                    if self.processed_count % save_interval == 0:\n","                        self._save_results(results, output_path)\n","                        gc.collect()\n","                        self._print_progress()\n","\n","            # å¤„ç†å‰©ä½™çš„é¡¹ç›®\n","            if batch:\n","                self._process_batch(batch, results)\n","\n","        # æœ€ç»ˆä¿å­˜\n","        self._save_results(results, output_path)\n","        self._print_final_stats()\n","\n","        return results\n","\n","    def _process_batch(self, batch: list, results: dict):\n","        \"\"\"å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®\"\"\"\n","        for qid, sample in batch:\n","            try:\n","                result = self._process_single_sample(qid, sample)\n","                if result:\n","                    results[qid] = result\n","                    self.processed_count += 1\n","\n","            except Exception as e:\n","                print(f\"âš ï¸ Error processing {qid}: {str(e)}\")\n","                self.error_count += 1\n","                continue\n","\n","    def _process_single_sample(self, qid: str, sample: dict) -> dict:\n","        \"\"\"å¤„ç†å•ä¸ªæ ·æœ¬\"\"\"\n","        question = sample[\"question\"]\n","        true_final_result = sample[\"true_final_result\"]\n","\n","        # æ‰¾åˆ°æ­£æ ·æœ¬\n","        correct_sampling_id = None\n","        correct_sample_answer = None\n","\n","        for sampling_id in [\"sampling0\", \"sampling1\", \"sampling2\"]:\n","            if sampling_id not in sample:\n","                continue\n","            if sample[sampling_id][\"final_result\"] == true_final_result:\n","                correct_sampling_id = sampling_id\n","                correct_sample_answer = sample[sampling_id][\"whole_answer\"]\n","                break\n","\n","        if correct_sample_answer is None:\n","            return None\n","\n","        sample_results = {}\n","\n","        # å¤„ç†è´Ÿæ ·æœ¬\n","        for sampling_id in [\"sampling0\", \"sampling1\", \"sampling2\"]:\n","            if sampling_id not in sample:\n","                continue\n","            sampling = sample[sampling_id]\n","\n","            # è·³è¿‡æ­£æ ·æœ¬\n","            if sampling[\"final_result\"] == true_final_result:\n","                continue\n","\n","            incorrect_sample_answer = sampling[\"whole_answer\"]\n","\n","            # è°ƒç”¨API\n","            prompt = build_error_prompt(question, correct_sample_answer, incorrect_sample_answer)\n","            output = call_custom_gpt_api(prompt)\n","\n","            # è§£æç»“æœ\n","            output = output.strip().strip(\"```\")\n","            if output.startswith(\"json\"):\n","                output = output[4:].strip()\n","\n","            output_json = json.loads(output)\n","\n","            # æŸ¥æ‰¾tokenç´¢å¼•\n","            error_sentence = output_json[\"first_error_sentence\"]\n","            fix_sentence = output_json[\"fix_sentence\"]\n","\n","            error_token_probs = sample[sampling_id][\"token_probs\"]\n","            correct_token_probs = sample[correct_sampling_id][\"token_probs\"]\n","\n","            error_begin_idx, error_end_idx = find_sentence_span_indices_robust(\n","                error_sentence, error_token_probs\n","            )\n","            fix_begin_idx, fix_end_idx = find_sentence_span_indices_robust(\n","                fix_sentence, correct_token_probs\n","            )\n","\n","            sample_results[sampling_id] = {\n","                \"first_error_sentence\": error_sentence,\n","                \"error_reason\": output_json[\"error_reason\"],\n","                \"fix_sentence\": fix_sentence,\n","                \"fix_reason\": output_json[\"fix_reason\"],\n","                \"correct_sampling_id\": correct_sampling_id,\n","                \"error_token_begin_index\": error_begin_idx,\n","                \"error_token_end_index\": error_end_idx,\n","                \"fix_token_begin_index\": fix_begin_idx,\n","                \"fix_token_end_index\": fix_end_idx\n","            }\n","\n","        return sample_results if sample_results else None\n","\n","    def _save_results(self, results: dict, output_path: str):\n","        \"\"\"ä¿å­˜ç»“æœåˆ°æ–‡ä»¶\"\"\"\n","        with open(output_path, 'w', encoding='utf-8') as f:\n","            json.dump(results, f, ensure_ascii=False, indent=2)\n","        print(f\"ğŸ’¾ Saved {len(results)} results to {output_path}\")\n","\n","    def _print_progress(self):\n","        \"\"\"æ‰“å°è¿›åº¦ä¿¡æ¯\"\"\"\n","        elapsed = time.time() - self.start_time\n","        speed = self.processed_count / elapsed if elapsed > 0 else 0\n","        print(f\"ğŸ“Š Progress: {self.processed_count} processed, {self.error_count} errors, \"\n","              f\"{speed:.2f} items/sec, {elapsed:.1f}s elapsed\")\n","\n","    def _print_final_stats(self):\n","        \"\"\"æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯\"\"\"\n","        elapsed = time.time() - self.start_time\n","        print(f\"\\nğŸ‰ Processing complete!\")\n","        print(f\"ğŸ“Š Total processed: {self.processed_count}\")\n","        print(f\"âš ï¸ Total errors: {self.error_count}\")\n","        print(f\"â±ï¸ Total time: {elapsed:.1f}s\")\n","        print(f\"ğŸš€ Average speed: {self.processed_count / elapsed:.2f} items/sec\")\n","\n","# ä½¿ç”¨ç¤ºä¾‹\n","def main():\n","    # é…ç½®\n","    API_KEY = \"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\"\n","    BASE_PATH = \"/content/drive/MyDrive/Cluster-proj\"\n","    range_tag = \"901-950\"\n","\n","    # è·¯å¾„è®¾ç½®\n","    input_json = f\"{BASE_PATH}/output/llm_steps/whole_logits/deepseek-math-7b-gsm-{range_tag}.json\"\n","    output_jsonl = f\"{BASE_PATH}/output/llm_steps/whole_logits/deepseek-math-7b-gsm-{range_tag}.jsonl\"\n","    output_results = f\"{BASE_PATH}/output/error_fix_index/deepseek-math-7b-{range_tag}_error_fix_index.json\"\n","\n","    # åˆ›å»ºå¤„ç†å™¨\n","    processor = JSONLProcessor(API_KEY)\n","\n","    # æ­¥éª¤1: è½¬æ¢ä¸ºJSONLï¼ˆå¦‚æœè¿˜æ²¡æœ‰è½¬æ¢ï¼‰\n","    if not os.path.exists(output_jsonl):\n","        processor.convert_json_to_jsonl(input_json, output_jsonl)\n","    else:\n","        print(f\"ğŸ“‚ JSONL file already exists: {output_jsonl}\")\n","\n","    # æ­¥éª¤2: å¤„ç†JSONLæ–‡ä»¶\n","    print(\"\\nğŸš€ Starting JSONL processing...\")\n","    results = processor.process_jsonl_file(\n","        jsonl_path=output_jsonl,\n","        output_path=output_results,\n","        batch_size=5,      # å‡å°æ‰¹æ¬¡å¤§å°ä»¥èŠ‚çœå†…å­˜\n","        save_interval=20   # æ¯20ä¸ªé¡¹ç›®ä¿å­˜ä¸€æ¬¡\n","    )\n","\n","    print(f\"\\nâœ… All processing complete! Results saved to {output_results}\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"oNoeI-O0WTdr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","import json\n","import os\n","import requests\n","from difflib import SequenceMatcher\n","import re\n","\n","\n","# âœ… Your custom API config\n","API_KEY = \"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\"\n","MODEL = \"gpt-4.1\"\n","\n","# âœ… Paths\n","start_index = 901\n","end_index = 950\n","range_tag = f\"{start_index}-{end_index}\"\n","\n","\n"],"metadata":{"id":"395ibCrtWL9g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"csyUtLNuTHGH","executionInfo":{"status":"ok","timestamp":1752385599756,"user_tz":-480,"elapsed":17615,"user":{"displayName":"Tingting Du","userId":"01262363838823204487"}},"outputId":"165775b8-e0e2-4420-ae86-528c9e9d77db"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["\n","BASE_PATH = \"/content/drive/MyDrive/Cluster-proj\"\n","LOGITS_PATH = f\"{BASE_PATH}/output/llm_steps/whole_logits/deepseek-math-7b-gsm-{range_tag}.json\"\n","\n","# âœ… Prompt builder\n","def build_error_prompt(question, true_whole_answer, sample_whole_answer):\n","    return f\"\"\"\n","Here is a math problem, its correct answer, and a sample answer that may contain mistakes.\n","\n","ã€Questionã€‘:\n","{question}\n","\n","ã€Correct Answerã€‘:\n","{true_whole_answer}\n","\n","ã€Incorrect Answerã€‘:\n","{sample_whole_answer}\n","\n","Please help me:\n","1. Identify the earliest mistake in the incorrect answer and provide the compelete sentence from that point.\n","2. Briefly explain why it is incorrect.\n","3. Find the fix sentence in correct answer that and fix the error.\n","4. Briefly explain why it can fix the error.\n","\n","Please output in the following JSON format:\n","{{\n","  \"first_error_sentence\": \"<sentence>\",\n","  \"error_reason\": \"<brief explanation>\",\n","  \"fix_sentence\": \"<sentence>\",\n","  \"fix_reason\": \"<brief explanation>\"\n","}}\n","\"\"\"\n","\n","\n","\n","# âœ… Call your custom GPT API\n","def call_custom_gpt_api(prompt):\n","    headers = {\n","        \"Authorization\": f\"Bearer {API_KEY}\",\n","        \"Content-Type\": \"application/json\"\n","    }\n","    payload = {\n","        \"model\": MODEL,\n","        \"messages\": [\n","            {\"role\": \"system\", \"content\": \"You are a meticulous and precise comparer.\"},\n","            {\"role\": \"user\", \"content\": prompt}\n","        ]\n","    }\n","    response = requests.post(\n","        \"https://api.openai.com/v1/chat/completions\",\n","        headers=headers,\n","        json=payload\n","    )\n","    if response.status_code != 200:\n","        raise Exception(f\"API request failed: {response.status_code}, {response.text}\")\n","    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n","\n"],"metadata":{"id":"zd7vIS3qv6rH","executionInfo":{"status":"ok","timestamp":1752385608551,"user_tz":-480,"elapsed":4,"user":{"displayName":"Tingting Du","userId":"01262363838823204487"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import re\n","# âœ… åŒ¹é…å‡½æ•°ï¼šè¿”å›ç‰‡æ®µèµ·æ­¢ token index\n","def find_sentence_span_indices_robust(fragment, token_probs):\n","    \"\"\"\n","    è¿”å› fragment åœ¨ token_probs ä¸­åŒ¹é…åˆ°çš„ token èŒƒå›´: (begin_index, end_index)\n","    - ä½¿ç”¨å»é™¤ç©ºç™½å­—ç¬¦çš„æ–¹å¼åŒ¹é…\n","    \"\"\"\n","    fragment_clean = re.sub(r\"\\s+\", \"\", fragment)\n","    tokens = [entry[\"token\"] for entry in token_probs]\n","    decoded_text = \"\".join(tokens)\n","    decoded_text_clean = re.sub(r\"\\s+\", \"\", decoded_text)\n","\n","    char_start_idx = decoded_text_clean.find(fragment_clean)\n","    if char_start_idx == -1:\n","        return -1, -1\n","\n","    cumulative_len = 0\n","    begin_index = -1\n","    for idx, entry in enumerate(token_probs):\n","        token_clean = re.sub(r\"\\s+\", \"\", entry[\"token\"])\n","        prev_len = cumulative_len\n","        cumulative_len += len(token_clean)\n","\n","        if begin_index == -1 and cumulative_len > char_start_idx:\n","            begin_index = idx\n","        if cumulative_len >= char_start_idx + len(fragment_clean):\n","            end_index = idx\n","            return begin_index, end_index\n","\n","    return begin_index, len(token_probs) - 1  # fallback"],"metadata":{"id":"kArxmbIiyzEo","executionInfo":{"status":"ok","timestamp":1752385611071,"user_tz":-480,"elapsed":3,"user":{"displayName":"Tingting Du","userId":"01262363838823204487"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!fallocate -l 4G /tmp/swapfile\n","!chmod 600 /tmp/swapfile\n","!mkswap /tmp/swapfile\n","!swapon /tmp/swapfile"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hcE4714PVqjA","executionInfo":{"status":"ok","timestamp":1752385892968,"user_tz":-480,"elapsed":435,"user":{"displayName":"Tingting Du","userId":"01262363838823204487"}},"outputId":"83fad770-27a4-430c-9c45-4d54a2168ae5"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Setting up swapspace version 1, size = 4 GiB (4294963200 bytes)\n","no label, UUID=0082653d-b97c-4cd0-9b1e-a8ce020a3a7e\n","swapon: /tmp/swapfile: swapon failed: Invalid argument\n"]}]},{"cell_type":"code","source":["\n","# âœ… Load logits_data\n","with open(LOGITS_PATH, \"r\") as f:\n","    logits_data = json.load(f)\n"],"metadata":{"id":"VivaNj7IvaWo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","results = {}\n","\n","# âœ… ä¸»å¾ªç¯ - ä¿®æ”¹ä¸ºæ¯”è¾ƒæ­£è´Ÿæ ·æœ¬\n","for qid, sample in logits_data.items():\n","    question = sample[\"question\"]\n","    true_final_result = sample[\"true_final_result\"]\n","\n","    # æ‰¾åˆ°æ­£æ ·æœ¬ï¼ˆæ­£ç¡®çš„samplingï¼‰\n","    correct_sampling_id = None\n","    correct_sample_answer = None\n","\n","    for sampling_id in [\"sampling0\", \"sampling1\", \"sampling2\"]:\n","        if sampling_id not in sample:\n","            continue\n","        if sample[sampling_id][\"final_result\"] == true_final_result:\n","            correct_sampling_id = sampling_id\n","            correct_sample_answer = sample[sampling_id][\"whole_answer\"]\n","            break\n","\n","    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ­£æ ·æœ¬ï¼Œè·³è¿‡è¿™ä¸ªé—®é¢˜\n","    if correct_sample_answer is None:\n","        print(f\"âš ï¸ No correct sampling found for {qid}, skipping...\")\n","        continue\n","\n","    # å¤„ç†è´Ÿæ ·æœ¬ï¼ˆé”™è¯¯çš„samplingï¼‰\n","    for sampling_id in [\"sampling0\", \"sampling1\", \"sampling2\"]:\n","        if sampling_id not in sample:\n","            continue\n","        sampling = sample[sampling_id]\n","\n","        # è·³è¿‡æ­£æ ·æœ¬\n","        if sampling[\"final_result\"] == true_final_result:\n","            continue\n","\n","        incorrect_sample_answer = sampling[\"whole_answer\"]\n","\n","        # æ„é€  prompt å¹¶è°ƒç”¨ API - ä½¿ç”¨æ­£æ ·æœ¬ä½œä¸ºæ­£ç¡®ç­”æ¡ˆ\n","        prompt = build_error_prompt(question, correct_sample_answer, incorrect_sample_answer)\n","        output = call_custom_gpt_api(prompt)\n","        print(f\"\\nğŸ” {qid} / {sampling_id} (comparing with {correct_sampling_id}):\\n{output}\")\n","\n","        # å»é™¤å¯èƒ½çš„ ''' åŒ…è£¹\n","        output = output.strip().strip(\"```\")\n","        if output.startswith(\"json\"):\n","            output = output[4:].strip()\n","\n","        # è§£æ JSON\n","        try:\n","            output_json = json.loads(output)\n","            error_sentence = output_json[\"first_error_sentence\"]\n","            error_reason = output_json[\"error_reason\"]\n","            fix_sentence = output_json[\"fix_sentence\"]\n","            fix_reason = output_json[\"fix_reason\"]\n","        except Exception as e:\n","            print(f\"âš ï¸ JSON parsing failed: {e}\")\n","            error_sentence = \"\"\n","            error_reason = output\n","            fix_sentence = \"\"\n","            fix_reason = \"\"\n","\n","        # âœ… ä¿å­˜ç»“æœ\n","        if qid not in results:\n","            results[qid] = {}\n","        results[qid][sampling_id] = {\n","            \"first_error_sentence\": error_sentence,\n","            \"error_reason\": error_reason,\n","            \"fix_sentence\": fix_sentence,\n","            \"fix_reason\": fix_reason,\n","            \"correct_sampling_id\": correct_sampling_id  # è®°å½•ä½¿ç”¨çš„æ­£æ ·æœ¬ID\n","        }\n"],"metadata":{"id":"VyVrVVLATnxN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# âœ… ç¬¬äºŒè½®éå†ï¼Œè¡¥å…… token index\n","for qid, sample_data in results.items():\n","    for sampling_id, info in sample_data.items():\n","        error_sentence = info[\"first_error_sentence\"]\n","        fix_sentence = info[\"fix_sentence\"]\n","        correct_sampling_id = info[\"correct_sampling_id\"]\n","\n","        # è·å–é”™è¯¯æ ·æœ¬çš„token_probs\n","        error_token_probs = logits_data[qid][sampling_id][\"token_probs\"]\n","\n","        # åŒ¹é…é”™è¯¯å¥å­çš„ token index èŒƒå›´\n","        error_begin_idx, error_end_idx = find_sentence_span_indices_robust(error_sentence, error_token_probs)\n","\n","        # è·å–æ­£ç¡®æ ·æœ¬çš„token_probs\n","        correct_token_probs = logits_data[qid][correct_sampling_id][\"token_probs\"]\n","\n","        # åŒ¹é…ä¿®å¤å¥å­çš„ token index èŒƒå›´\n","        fix_begin_idx, fix_end_idx = find_sentence_span_indices_robust(fix_sentence, correct_token_probs)\n","\n","        # åŠ å…¥åˆ°ç»“æœä¸­\n","        info[\"error_token_begin_index\"] = error_begin_idx\n","        info[\"error_token_end_index\"] = error_end_idx\n","        info[\"fix_token_begin_index\"] = fix_begin_idx\n","        info[\"fix_token_end_index\"] = fix_end_idx\n","\n","        # å¯é€‰ï¼šæ‰“å°æ£€æŸ¥\n","        print(f\"{qid} / {sampling_id}:\")\n","        print(f\"  Error: [{error_begin_idx}, {error_end_idx}] : {error_sentence}\")\n","        print(f\"  Fix (from {correct_sampling_id}): [{fix_begin_idx}, {fix_end_idx}] : {fix_sentence}\")\n","\n","# âœ… ä¿å­˜ç»“æœ\n","output_dir = os.path.join(BASE_PATH, \"output/error_fix_index\")\n","os.makedirs(output_dir, exist_ok=True)\n","output_path = os.path.join(output_dir, f\"deepseek-math-7b-{range_tag}_error_fix_index.json\")\n","with open(output_path, \"w\") as f:\n","    json.dump(results, f, ensure_ascii=False, indent=2)\n","\n","print(f\"\\nâœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° {output_path}\")\n"],"metadata":{"id":"PO1s4NmFTpnx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# âœ… éªŒè¯éƒ¨åˆ† - ä¿®æ”¹ä¸ºéªŒè¯é”™è¯¯å¥å­å’Œä¿®å¤å¥å­\n","print(\"\\n\" + \"=\"*80)\n","print(\"å¼€å§‹éªŒè¯...\")\n","\n","for qid, sample_data in results.items():\n","    for sampling_id, info in sample_data.items():\n","        error_sentence = info.get(\"first_error_sentence\", \"\").strip()\n","        error_start = info.get(\"error_token_begin_index\", -1)\n","        error_end = info.get(\"error_token_end_index\", -1)\n","\n","        fix_sentence = info.get(\"fix_sentence\", \"\").strip()\n","        fix_start = info.get(\"fix_token_begin_index\", -1)\n","        fix_end = info.get(\"fix_token_end_index\", -1)\n","\n","        correct_sampling_id = info.get(\"correct_sampling_id\", \"\")\n","\n","        # éªŒè¯é”™è¯¯å¥å­\n","        if error_start != -1 and error_end != -1:\n","            error_token_probs = logits_data[qid][sampling_id][\"token_probs\"]\n","            error_tokens = [entry[\"token\"] for entry in error_token_probs[error_start:error_end+1]]\n","            error_reconstructed = \" \".join(error_tokens).strip()\n","\n","            print(\"=\" * 60)\n","            print(f\"ğŸ” {qid} / {sampling_id}\")\n","            print(f\"ğŸ“Œ Error token span [{error_start}, {error_end}]:\\n{error_reconstructed}\")\n","            print(f\"ğŸ“Œ Error sentence:\\n{error_sentence}\")\n","\n","            error_clean = re.sub(r\"\\s+\", \"\", error_reconstructed.lower())\n","            error_sentence_clean = re.sub(r\"\\s+\", \"\", error_sentence.lower())\n","            error_match = \"âœ… MATCH\" if error_clean == error_sentence_clean else \"âŒ DIFFERENT\"\n","            print(f\"ğŸ” Erroræ¯”å¯¹ç»“æœ: {error_match}\")\n","\n","        # éªŒè¯ä¿®å¤å¥å­\n","        if fix_start != -1 and fix_end != -1:\n","            correct_token_probs = logits_data[qid][correct_sampling_id][\"token_probs\"]\n","            fix_tokens = [entry[\"token\"] for entry in correct_token_probs[fix_start:fix_end+1]]\n","            fix_reconstructed = \" \".join(fix_tokens).strip()\n","\n","            print(f\"\\nğŸ“Œ Fix token span (from {correct_sampling_id}) [{fix_start}, {fix_end}]:\\n{fix_reconstructed}\")\n","            print(f\"ğŸ“Œ Fix sentence:\\n{fix_sentence}\")\n","\n","            fix_clean = re.sub(r\"\\s+\", \"\", fix_reconstructed.lower())\n","            fix_sentence_clean = re.sub(r\"\\s+\", \"\", fix_sentence.lower())\n","            fix_match = \"âœ… MATCH\" if fix_clean == fix_sentence_clean else \"âŒ DIFFERENT\"\n","            print(f\"ğŸ” Fixæ¯”å¯¹ç»“æœ: {fix_match}\\n\")"],"metadata":{"id":"r9qVXoMsTq_t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#version 1"],"metadata":{"id":"QnM5ucqEWSFa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = {}\n","\n","# âœ… ä¸»å¾ªç¯\n","for qid, sample in logits_data.items():\n","    question = sample[\"question\"]\n","    true_final_result = sample[\"true_final_result\"]\n","    # true_whole_answer = sample[\"true_whole_answer\"]\n","\n","    for sampling_id in [\"sampling0\", \"sampling1\", \"sampling2\"]:\n","        if sampling_id not in sample:\n","            continue\n","        sampling = sample[sampling_id]\n","        if sampling[\"final_result\"] == true_final_result:\n","            true_whole_answer = sampling[\"whole_answer\"]\n","\n","        sample_whole_answer = sampling[\"whole_answer\"]\n","\n","        # æ„é€  prompt å¹¶è°ƒç”¨ API\n","        prompt = build_error_prompt(question, true_whole_answer, sample_whole_answer)\n","        output = call_custom_gpt_api(prompt)\n","        print(f\"\\nğŸ” {qid} / {sampling_id}:\\n{output}\")\n","\n","        # å»é™¤å¯èƒ½çš„ ''' åŒ…è£¹\n","        output = output.strip().strip(\"\")\n","        if output.startswith(\"json\"):\n","          output = output[4:].strip()\n","        # è§£æ JSON\n","        try:\n","            output_json = json.loads(output)\n","            sentence = output_json[\"first_error_sentence\"]\n","            error_reason = output_json[\"error_reason\"]\n","        except Exception as e:\n","            print(f\"âš ï¸ JSON parsing failed: {e}\")\n","            sentence = \"\"\n","            error_reason = output\n","\n","        # âœ… ä¿å­˜ç»“æœï¼ˆå­—æ®µåä¸º sentenceï¼‰\n","        if qid not in results:\n","            results[qid] = {}\n","        results[qid][sampling_id] = {\n","            \"first_error_sentence\": sentence,\n","            \"error_reason\": error_reason,\n","        }"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"id":"SFgdVELt0y2b","executionInfo":{"status":"error","timestamp":1752386044311,"user_tz":-480,"elapsed":68,"user":{"displayName":"Tingting Du","userId":"01262363838823204487"}},"outputId":"e3de0332-bcf9-49b7-8b1b-3e3622bde60c"},"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'logits_data' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2-3590688224.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# âœ… ä¸»å¾ªç¯\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mqid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlogits_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrue_final_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"true_final_result\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'logits_data' is not defined"]}]},{"cell_type":"code","source":["\n","\n","# âœ… ç¬¬äºŒè½®éå†ï¼Œè¡¥å…… token indexï¼ˆä¸é‡æ–°è°ƒç”¨ APIï¼‰\n","for qid, sample_data in results.items():\n","    for sampling_id, info in sample_data.items():\n","        sentence = info[\"first_error_sentence\"]\n","        token_probs = logits_data[qid][sampling_id][\"token_probs\"]\n","\n","        # åŒ¹é… token index èŒƒå›´\n","        begin_idx, end_idx = find_sentence_span_indices_robust(sentence, token_probs)\n","\n","        # åŠ å…¥åˆ°ç»“æœä¸­\n","        info[\"first_error_token_index\"] = begin_idx\n","        info[\"last_error_token_index\"] = end_idx\n","\n","        # å¯é€‰ï¼šæ‰“å°æ£€æŸ¥\n","        print(f\"{qid} / {sampling_id} â†’ [{begin_idx}, {end_idx}] : {sentence}\")"],"metadata":{"id":"-rzUgTjDvZW2","executionInfo":{"status":"ok","timestamp":1752386044376,"user_tz":-480,"elapsed":42,"user":{"displayName":"Tingting Du","userId":"01262363838823204487"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["output_dir = os.path.join(BASE_PATH, \"output/error_index\")\n","os.makedirs(output_dir, exist_ok=True)\n","output_path = os.path.join(output_dir, f\"deepseek-math-7b-{range_tag}_index.json\")\n","with open(output_path, \"w\") as f:\n","    json.dump(results, f, ensure_ascii=False, indent=2)\n","\n","print(f\"\\nâœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° {output_path}\")\n"],"metadata":{"id":"r5s9ar40087_","executionInfo":{"status":"ok","timestamp":1752318406170,"user_tz":-480,"elapsed":22,"user":{"displayName":"Tingting Du","userId":"01262363838823204487"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1e62a26f-071f-400c-a71c-df052641eca8"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","âœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° /content/drive/MyDrive/Cluster-proj/output/error_index/deepseek-math-7b-901-950_index.json\n"]}]},{"cell_type":"code","source":["#test\n","logits_data['q_947'].keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TJo4IlCJVNzi","executionInfo":{"status":"ok","timestamp":1752318668660,"user_tz":-480,"elapsed":9,"user":{"displayName":"Tingting Du","userId":"01262363838823204487"}},"outputId":"7aef6f02-a7bc-49c2-819b-3e908f350525"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['question', 'true_whole_answer', 'true_final_result', 'sampling0', 'sampling1', 'sampling2'])"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["#test\n","logits_data['q_927']['true_final_result']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"KQ9kOdCfVPqB","executionInfo":{"status":"ok","timestamp":1752319077529,"user_tz":-480,"elapsed":14,"user":{"displayName":"Tingting Du","userId":"01262363838823204487"}},"outputId":"77515932-f71b-4be8-fbe7-a87b9c6cb6cf"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'31'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["#test\n","logits_data['q_927']['sampling2']['final_result']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"b7d33zosVUVN","executionInfo":{"status":"ok","timestamp":1752319096785,"user_tz":-480,"elapsed":17,"user":{"displayName":"Tingting Du","userId":"01262363838823204487"}},"outputId":"06e8a2a1-6ced-443e-833f-c6ad0b0f0144"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'50'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["#check\n","\n","\n","import json\n","import os\n","import re\n","\n","# âœ… è·¯å¾„é…ç½®\n","BASE_PATH = \"/content/drive/MyDrive/Cluster-proj\"\n","range_tag = \"901-950\"\n","LOGITS_PATH = f\"{BASE_PATH}/output/llm_steps/whole_logits/deepseek7b-gsm-{range_tag}.json\"\n","ERROR_INDEX_PATH = f\"{BASE_PATH}/output/error_index/deepseek-math-7b-{range_tag}_index.json\"\n","\n","# # âœ… åŠ è½½ä¸¤ä¸ªæ•°æ®æº\n","# with open(LOGITS_PATH, \"r\") as f:\n","#     logits_data = json.load(f)\n","\n","with open(ERROR_INDEX_PATH, \"r\") as f:\n","    error_index_data = json.load(f)\n","\n","# âœ… éå†æ¯ä¸€æ¡ï¼Œæ‹¼æ¥ token & æ¯”å¯¹å¥å­\n","for qid, sample_data in error_index_data.items():\n","    for sampling_id, info in sample_data.items():\n","        sentence = info.get(\"first_error_sentence\", \"\").strip()\n","        start = info.get(\"first_error_token_index\", -1)\n","        end = info.get(\"last_error_token_index\", -1)\n","\n","        if start == -1 or end == -1:\n","            print(f\"{qid} / {sampling_id} âŒ ç¼ºå¤± index\")\n","            continue\n","\n","        token_probs = logits_data[qid][sampling_id][\"token_probs\"]\n","        tokens = [entry[\"token\"] for entry in token_probs[start:end+1]]\n","        reconstructed = \" \".join(tokens).strip()\n","\n","        print(\"=\" * 60)\n","        print(f\"ğŸ” {qid} / {sampling_id}\")\n","        print(f\"ğŸ“Œ Token span [{start}, {end}]:\\n{reconstructed}\")\n","        print(f\"\\nğŸ“Œ Error sentence:\\n{sentence}\")\n","\n","        # ç®€å•æ¯”å¯¹ç›¸ä¼¼åº¦\n","        reconstructed_clean = re.sub(r\"\\s+\", \"\", reconstructed.lower())\n","        sentence_clean = re.sub(r\"\\s+\", \"\", sentence.lower())\n","        match_status = \"âœ… MATCH\" if reconstructed_clean == sentence_clean else \"âŒ DIFFERENT\"\n","\n","        print(f\"\\nğŸ” æ¯”å¯¹ç»“æœ: {match_status}\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"viclNJ79EZF5","executionInfo":{"status":"ok","timestamp":1752318455467,"user_tz":-480,"elapsed":176,"user":{"displayName":"Tingting Du","userId":"01262363838823204487"}},"outputId":"ea17c979-4c28-4438-ee46-e2de862d2b5a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","ğŸ” q_902 / sampling0\n","ğŸ“Œ Token span [36, 74]:\n","He fed an equal number of straw s to the pig lets , which means he fed  1 8 0 straw s /  2 0 pig lets =  9 straw s to each pig let .\n","\n","ğŸ“Œ Error sentence:\n","He fed an equal number of straws to the piglets, which means he fed 180 straws / 20 piglets = 9 straws to each piglet.\n","\n","ğŸ” æ¯”å¯¹ç»“æœ: âœ… MATCH\n","\n","============================================================\n","ğŸ” q_906 / sampling2\n","ğŸ“Œ Token span [34, 56]:\n","At the second stop ,  3 people got off the bus , so the number of passengers decreased by  3 .\n","\n","ğŸ“Œ Error sentence:\n","At the second stop, 3 people got off the bus, so the number of passengers decreased by 3.\n","\n","ğŸ” æ¯”å¯¹ç»“æœ: âœ… MATCH\n","\n","============================================================\n","ğŸ” q_911 / sampling0\n","ğŸ“Œ Token span [136, 182]:\n","Now , to find the combined time the all igators walked , we need to add the time Paul spent walking to the Nile Delta ( 4 hours ) and the time the other six all igators spent walking on the return journey ( 6 hours ).\n","\n","ğŸ“Œ Error sentence:\n","Now, to find the combined time the alligators walked, we need to add the time Paul spent walking to the Nile Delta (4 hours) and the time the other six alligators spent walking on the return journey (6 hours).\n","\n","ğŸ” æ¯”å¯¹ç»“æœ: âœ… MATCH\n","\n","============================================================\n","ğŸ” q_911 / sampling1\n","ğŸ“Œ Token span [117, 183]:\n","3 . The total time the all igators walked is the sum of the time it took for Paul to walk to the Nile Delta and the time it took for Paul and the other six all igators to travel from the Nile Delta to their home . So , the total time is  4 hours +  6 hours =  1 0 hours .\n","\n","ğŸ“Œ Error sentence:\n","3. The total time the alligators walked is the sum of the time it took for Paul to walk to the Nile Delta and the time it took for Paul and the other six alligators to travel from the Nile Delta to their home. So, the total time is 4 hours + 6 hours = 10 hours.\n","\n","ğŸ” æ¯”å¯¹ç»“æœ: âœ… MATCH\n","\n","============================================================\n","ğŸ” q_911 / sampling2\n","ğŸ“Œ Token span [168, 197]:\n","Now , to find the total time the all igators walked , we add the time it took for the journey to the Nile Delta and the return journey .\n","\n","ğŸ“Œ Error sentence:\n","Now, to find the total time the alligators walked, we add the time it took for the journey to the Nile Delta and the return journey.\n","\n","ğŸ” æ¯”å¯¹ç»“æœ: âœ… MATCH\n","\n","============================================================\n","ğŸ” q_912 / sampling0\n","ğŸ“Œ Token span [176, 235]:\n","So , they can make  1 6 /  7 =  2 jars of jam from Betty ' s strawberries ,  3 6 /  7 =  5 jars of jam from Matthew ' s strawberries , and  1 8 /  7 =  2 jars of jam from Natalie ' s strawberries .\n","\n","ğŸ“Œ Error sentence:\n","So, they can make 16 / 7 = 2 jars of jam from Betty's strawberries, 36 / 7 = 5 jars of jam from Matthew's strawberries, and 18 / 7 = 2 jars of jam from Natalie's strawberries.\n","\n","ğŸ” æ¯”å¯¹ç»“æœ: âœ… MATCH\n","\n","============================================================\n","ğŸ” q_919 / sampling0\n","ğŸ“Œ Token span [47, 75]:\n","She then adds twice the amount she already had , so she adds  2 *  1 0 0 0 =  2 0 0 0 songs .\n","\n","ğŸ“Œ Error sentence:\n","She then adds twice the amount she already had, so she adds 2 * 1000 = 2000 songs.\n","\n","ğŸ” æ¯”å¯¹ç»“æœ: âœ… MATCH\n","\n","============================================================\n","ğŸ” q_919 / sampling1\n","ğŸ“Œ Token span [50, 82]:\n","She then adds twice the amount she already had on her mp 3 player , which is  2 *  1 0 0 0 =  2 0 0 0 songs .\n","\n","ğŸ“Œ Error sentence:\n","She then adds twice the amount she already had on her mp3 player, which is 2 * 1000 = 2000 songs.\n","\n","ğŸ” æ¯”å¯¹ç»“æœ: âœ… MATCH\n","\n","============================================================\n","ğŸ” q_919 / sampling2\n","ğŸ“Œ Token span [47, 74]:\n","She then adds twice the amount she already had , which is  2 *  1 0 0 0 =  2 0 0 0 songs .\n","\n","ğŸ“Œ Error sentence:\n","She then adds twice the amount she already had, which is 2 * 1000 = 2000 songs.\n","\n","ğŸ” æ¯”å¯¹ç»“æœ: âœ… MATCH\n","\n","============================================================\n","ğŸ” q_922 / sampling0\n","ğŸ“Œ Token span [101, 122]:\n","In the new company , she ' s earned  2 0 % more than she earned at the old company .\n","\n","ğŸ“Œ Error sentence:\n","In the new company, she's earned 20% more than she earned at the old company.\n","\n","ğŸ” æ¯”å¯¹ç»“æœ: âœ… MATCH\n","\n","============================================================\n","ğŸ” q_927 / sampling0\n","ğŸ“Œ Token span [21, 49]:\n","So , each friend invited one person , making the total number of guests Ashley had initially  2 0 +  2 0 =  4 0 .\n","\n","ğŸ“Œ Error sentence:\n","So, each friend invited one person, making the total number of guests Ashley had initially 20 + 20 = 40.\n","\n","ğŸ” æ¯”å¯¹ç»“æœ: âœ… MATCH\n","\n","============================================================\n","ğŸ” q_927 / sampling1\n","ğŸ“Œ Token span [23, 56]:\n","So , each of the  2 0 friends brought one more person , which means there were  2 0 *  1 =  2 0 additional people at the party .\n","\n","ğŸ“Œ Error sentence:\n","So, each of the 20 friends brought one more person, which means there were 20 * 1 = 20 additional people at the party.\n","\n","ğŸ” æ¯”å¯¹ç»“æœ: âœ… MATCH\n","\n","============================================================\n","ğŸ” q_927 / sampling2\n","ğŸ“Œ Token span [75, 123]:\n","Therefore , the total number of people at the party , including Ashley , is  2 0 ( Ash ley ) +  2 0 ( her friends ) +  1 0 ( the extra people brought by her friends ) =  5 0 people .\n","\n","ğŸ“Œ Error sentence:\n","Therefore, the total number of people at the party, including Ashley, is 20 (Ashley) + 20 (her friends) + 10 (the extra people brought by her friends) = 50 people.\n","\n","ğŸ” æ¯”å¯¹ç»“æœ: âœ… MATCH\n","\n","============================================================\n","ğŸ” q_943 / sampling1\n","ğŸ“Œ Token span [96, 122]:\n","He wants to reach a goal of $ 9 6 , so we can set up the equation  1 2 D =  9 6 .\n","\n","ğŸ“Œ Error sentence:\n","He wants to reach a goal of $96, so we can set up the equation 12D = 96.\n","\n","ğŸ” æ¯”å¯¹ç»“æœ: âœ… MATCH\n","\n","============================================================\n","ğŸ” q_947 / sampling0\n","ğŸ“Œ Token span [33, 49]:\n","This means that in  2 0 days , he will have doubled his current practice .\n","\n","ğŸ“Œ Error sentence:\n","This means that in 20 days, he will have doubled his current practice.\n","\n","ğŸ” æ¯”å¯¹ç»“æœ: âœ… MATCH\n","\n","============================================================\n","ğŸ” q_947 / sampling1\n","ğŸ“Œ Token span [63, 78]:\n","This means that Johnny has done  1 0 days worth of practice so far .\n","\n","ğŸ“Œ Error sentence:\n","This means that Johnny has done 10 days worth of practice so far.\n","\n","ğŸ” æ¯”å¯¹ç»“æœ: âœ… MATCH\n","\n"]}]}]}
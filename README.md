
# ğŸ§  Step-wise Confidence Clustering for Reasoning Quality Analysis

This project explores how step-by-step outputs generated by Large Language Models (LLMs) can be analyzed at the **token level**, categorized into three clusters: **Named Enitity**, **Logical Connectors**, and **Semantic Roles**. The experiment is built upon reasoning outputs generated by the `Qwen\Qwen2.5-7B`,`deepseek-ai/DeepSeek-R1-Distill-Qwen-7B` model across three decoding strategies.

---

## ğŸ“ Project Structure

```
Cluster/
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ hotpotQA/                        # Input: HotpotQA-style data
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ DeepSeek-7B-sample71-80_with_all_steps.json
â”‚   â”œâ”€â”€ DeepSeek-7B-sample81-90_with_all_steps.json
â”‚   â”œâ”€â”€ Qwen2.5-7B-sample21-30_with_all_steps.json
â”‚   â””â”€â”€ Qwen2.5-7B-sample31-40_with_all_steps.json
â””â”€â”€ notebooks/
    â””â”€â”€ cluster_confidence.ipynb        # categorize step-wise token into clusters
    â””â”€â”€ generate_baseline_q.ipynb        # Full generation and analysis pipeline
```

---
### ğŸ”„ Workflow Summary:

## ğŸ”Phase 1: Generate LLM's answers to questions (generate_baseline_q.ipynb)

0. **Data Source**  
  - Format: HotpotQA (multi-hop QA)
  - Samples: Small batch selection (10 per model segment)
  - Fields used: `question`, `context`, `supporting_facts`, `answer`, `type`, `level`

1. **Load Model**  
   - Uses `Qwen/Qwen-2.5-7B`,`deepseek-ai/DeepSeek-R1-Distill-Qwen-7B` via HuggingFace Transformers, run through Google Colab
   - Loaded in `float16` with `device_map="auto"`

2. **Prompt Design**  
   - Prompts include:  
     ```
     //input question and whole context, hide answer and supporting fact
     Please reason through the question step-by-step using a structured format.
     Each step must begin with Step i...
     ```
   - Outputs are limited to 300 tokens

3. **Decoding Strategies**  
   - `greedy`, `beam search (3 beams)`, and `top-k/top-p sampling(temperature=0.7)`

4. **Token Probability Extraction**  
   - `output.scores` from `.generate()` are used to get logit scores
   - Per-token probabilities are computed using softmax

5. **Answer + Token Probs Output**  
   - Stored as:
     ```markdown
     "token_probs": [["token1", prob1], ["token2", prob2], ...]
     ```

6. **Step Extraction**  
   - Function: `extract_steps_from_token_probs`
   - Segments outputs using `Step i:` markers

7. **Output Format**  
   - Stored per decoding mode:
     - `greedy`, `beam`, `sampling`
   - Enriched with step-wise breakdowns:
     - `greedy_step_token_probs`, etc.

8. **Output Files**  
   - Written to `.json` under `outputs/`:
     - `DeepSeek-7B-sampleXX-YY_with_all_steps.json`

## ğŸ”Phase 2: Token Clustering (cluster_confidence.ipynb)

### Categories:
- **Symbol**: punctuation, numbers, `Step`, etc.
- **LogicalConnector**: e.g., `because`, `however`, `therefore`
- **Entity:** named entities from spaCy NER (e.g., `Entity:PERSON`)
- **Syntactic:** dependency roles from spaCy (e.g., `Syntactic:nsubj`)
- **Other**: fallback for uncategorized tokens

### Cluster Scoring:
For each cluster in a step:
- `total_prob`: sum of token probabilities
- `avg_prob`: mean token probability
- `top_token`: highest-probability token

---


## ğŸ“ Phase 1 Output File Structure Example

```markdown
{
  "<example_id>": {
    "question": "What profession do Am Rong and Alexandre Rockwell have in common?",
    "support_fact": [["Am Rong", 0], ["Alexandre Rockwell", 2]],
    "context": [
      ["Somebody to Love (film)", [...]],
      ["Four Rooms", [...]],
      ...
    ],
    "true_ans": "filmmaker",
    "type": "comparison",
    "level": "medium",

    //Raw Generation Outputs:

    "greedy": {
      "answer": "...",
      "token_probs": [["token1", prob1], ["token2", prob2], ...]
    },
    "beam": {...},
    "sampling": {...},

    //Step-Wise Extraction:

    "greedy_step_token_probs": {
      "1": [{"token": "Step", "prob": 0.98}, ...], //Step 1
      "2": [...],                                  //Step 2
      ...
    },
    "beam_step_token_probs": {...},
    "sampling_step_token_probs": {...}
  }
}
```

---
## ğŸ§ª Key Components

### 3. **Question, Context, Answer from dataset**
- `question`
- `support_fact`: A list of supporting entity mentions and sentence indices.
- `context`: A list of context paragraphs. Each element is a pair:
  - Title (string)
  - List of sentences (strings)
- `true_ans`: the ground truth answer
- `type`: e.g. comparison, bridge
- `level`: e.g. hard, medium
  
### 1. **Raw Generation Outputs**
- For each decoding mode (`greedy`, `beam`, `sampling`), the following is stored:
  - `answer`: Full generated answer (string)
  - `token_probs`: List of tokens and their generation probabilities

### 2. **Step-Wise Extraction**
- Tokens are grouped into steps using `Step i:` markers.
- Result is stored under keys like `greedy_step_token_probs`.


## ğŸ’¡ Future Extensions
- Add step-level correctness supervision


## ğŸ›  Tools Used

- Model inference: HuggingFace Transformers (DeepSeek, Qwen)
- Linguistic parsing: spaCy (NER + Dependency Parsing)
- Data processing: Python, pandas, json

## ğŸ“š Acknowledgements

- [HotpotQA](https://hotpotqa.github.io)
- [DeepSeek-AI](https://huggingface.co/deepseek-ai)
- [Qwen by Alibaba](https://huggingface.co/Qwen)
- [spaCy NLP](https://spacy.io/)




